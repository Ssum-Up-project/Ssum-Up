{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gy_data_processing_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a39fcbc79ab04231b41ef4481d344db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43dcc835a7db4efe9b3a87790bfc08d9",
              "IPY_MODEL_f32d6f1bdd2f44c0ab831c4568ceab50",
              "IPY_MODEL_cdb02bc9896c4def9834342fbeab071e"
            ],
            "layout": "IPY_MODEL_bbdaaf6dd1f84f41b9c1068809ca1991"
          }
        },
        "43dcc835a7db4efe9b3a87790bfc08d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a622b121b41246b182407b4e30bd98cd",
            "placeholder": "​",
            "style": "IPY_MODEL_877e54bb51544ec398bc4019e4551e6d",
            "value": "Downloading: 100%"
          }
        },
        "f32d6f1bdd2f44c0ab831c4568ceab50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b9041148d5455ab9293e8f0283bbcd",
            "max": 1246,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cd5524361b544c1a05466fccf7887b8",
            "value": 1246
          }
        },
        "cdb02bc9896c4def9834342fbeab071e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e9fdb7577894f2cbd2fb61fa7249d76",
            "placeholder": "​",
            "style": "IPY_MODEL_7903353e2b9c43718c6fe9341acfa405",
            "value": " 1.22k/1.22k [00:00&lt;00:00, 25.5kB/s]"
          }
        },
        "bbdaaf6dd1f84f41b9c1068809ca1991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a622b121b41246b182407b4e30bd98cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "877e54bb51544ec398bc4019e4551e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39b9041148d5455ab9293e8f0283bbcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cd5524361b544c1a05466fccf7887b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e9fdb7577894f2cbd2fb61fa7249d76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7903353e2b9c43718c6fe9341acfa405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c73433063e7c41fdb0fd3d0061e58209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39eb4ebf664342cc800cd8a2324738c7",
              "IPY_MODEL_07552a07f1f7440da8abecb08f88f902",
              "IPY_MODEL_f031eb2ba23d4a29b6b8bc9f2d61b3b6"
            ],
            "layout": "IPY_MODEL_9c97cef06644416aa64bba7f4ad0ef64"
          }
        },
        "39eb4ebf664342cc800cd8a2324738c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14574c1addec48cf9da45ebd1b3fa217",
            "placeholder": "​",
            "style": "IPY_MODEL_40fddec9bd3748b6b61b1c40130b93c6",
            "value": "Downloading: 100%"
          }
        },
        "07552a07f1f7440da8abecb08f88f902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18ea2a1411ed4f548edd50d1f3cd6356",
            "max": 435701303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1688fb8d9da4bc981ec8ff9343db7f9",
            "value": 435701303
          }
        },
        "f031eb2ba23d4a29b6b8bc9f2d61b3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14e62d063704a48bae08f222644942d",
            "placeholder": "​",
            "style": "IPY_MODEL_478e4042719b4f0184630af6ddc10295",
            "value": " 416M/416M [00:13&lt;00:00, 34.2MB/s]"
          }
        },
        "9c97cef06644416aa64bba7f4ad0ef64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14574c1addec48cf9da45ebd1b3fa217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40fddec9bd3748b6b61b1c40130b93c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18ea2a1411ed4f548edd50d1f3cd6356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1688fb8d9da4bc981ec8ff9343db7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c14e62d063704a48bae08f222644942d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "478e4042719b4f0184630af6ddc10295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d74ba00ecc9d4aaf9efaa39f292b4108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_855227810065474a8fb657eda383867f",
              "IPY_MODEL_72b817c09afc4d679fc83d220e27c740",
              "IPY_MODEL_9df89c6b76ea4a34827e7cd061b55ec0"
            ],
            "layout": "IPY_MODEL_645218a39da44828bcd7de76e48f85a7"
          }
        },
        "855227810065474a8fb657eda383867f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0105fc0d73334c33a067ec3f500837ba",
            "placeholder": "​",
            "style": "IPY_MODEL_a99b628059dc4eda9f09614f501f2018",
            "value": "Downloading: 100%"
          }
        },
        "72b817c09afc4d679fc83d220e27c740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe4c22fb3394d93ab1190815ecec19c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ef27b825988423daf3607a9ba8f847b",
            "value": 231508
          }
        },
        "9df89c6b76ea4a34827e7cd061b55ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025027abaa6a4113910f3a9f8599c796",
            "placeholder": "​",
            "style": "IPY_MODEL_1164f20809c14009892ded7b79b5467f",
            "value": " 226k/226k [00:00&lt;00:00, 651kB/s]"
          }
        },
        "645218a39da44828bcd7de76e48f85a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0105fc0d73334c33a067ec3f500837ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99b628059dc4eda9f09614f501f2018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe4c22fb3394d93ab1190815ecec19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef27b825988423daf3607a9ba8f847b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "025027abaa6a4113910f3a9f8599c796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1164f20809c14009892ded7b79b5467f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f9e360354e64a1fae1e68ea062a7b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39882f204c894d91b9f3d430ae21bd79",
              "IPY_MODEL_660612611caa426e8f9b25651c2333c1",
              "IPY_MODEL_cd0ba90d56ec4af9b0f3b055c5216f10"
            ],
            "layout": "IPY_MODEL_65f941e684f34b96b7519696cb8c20b0"
          }
        },
        "39882f204c894d91b9f3d430ae21bd79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d367df6a0c804b51911a23a954fefc75",
            "placeholder": "​",
            "style": "IPY_MODEL_eb2befec138b45098681f11713c5adba",
            "value": "Downloading: 100%"
          }
        },
        "660612611caa426e8f9b25651c2333c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a02127d790ea4b75b7002fb303e5176a",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f32ceadb336433ab762fd0d90daec81",
            "value": 112
          }
        },
        "cd0ba90d56ec4af9b0f3b055c5216f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f724ab0ec54b45f39a14a3b87c5f82e3",
            "placeholder": "​",
            "style": "IPY_MODEL_4b0f8319351641ed88fc9798e8f00b53",
            "value": " 112/112 [00:00&lt;00:00, 2.34kB/s]"
          }
        },
        "65f941e684f34b96b7519696cb8c20b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d367df6a0c804b51911a23a954fefc75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb2befec138b45098681f11713c5adba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a02127d790ea4b75b7002fb303e5176a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f32ceadb336433ab762fd0d90daec81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f724ab0ec54b45f39a14a3b87c5f82e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b0f8319351641ed88fc9798e8f00b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9897889b41114c97bd661e19058705b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c71335cc762848cdaa6b2c8ca7357162",
              "IPY_MODEL_a0c800df3ebf4483b9fe15ad41ecc80f",
              "IPY_MODEL_df87cfa898394818a2858050ee5cd06b"
            ],
            "layout": "IPY_MODEL_45bac38fada242ceb7e0336f16a16ca8"
          }
        },
        "c71335cc762848cdaa6b2c8ca7357162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce0a956dc8d4c98b4cd3d2813e34f41",
            "placeholder": "​",
            "style": "IPY_MODEL_9c100b132a3f4b758047d47dded74e5c",
            "value": "Downloading: 100%"
          }
        },
        "a0c800df3ebf4483b9fe15ad41ecc80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f5d28f875504775b4d5c8e186d84120",
            "max": 530,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36bd7be61e0d472eb6d9a564e266e2d1",
            "value": 530
          }
        },
        "df87cfa898394818a2858050ee5cd06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df8aeed6d2d04a338ee2f40e59aa1af3",
            "placeholder": "​",
            "style": "IPY_MODEL_0a7ac03edd8c48288048184f246994a1",
            "value": " 530/530 [00:00&lt;00:00, 13.2kB/s]"
          }
        },
        "45bac38fada242ceb7e0336f16a16ca8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce0a956dc8d4c98b4cd3d2813e34f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c100b132a3f4b758047d47dded74e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f5d28f875504775b4d5c8e186d84120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36bd7be61e0d472eb6d9a564e266e2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df8aeed6d2d04a338ee2f40e59aa1af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7ac03edd8c48288048184f246994a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 작업내용\n",
        "\n",
        "## 1. 영상에서 자막 추출\n",
        "\n",
        "## 2. 전처리 자막 생성\n",
        "1.  괄호로 들어가는 보조 설명 제거   \n",
        " ex. [Music]\n",
        "2. 구어에서 발생하는 filler words 제거\n",
        "3. 반복단어 제거\n",
        "4. 문장 끝점 추가(rpunct)\n",
        "\n",
        "## 3. 요약모델 테스트\n",
        "1. \"facebook/bart-large-cnn\" 모델\n",
        "2. \"sshleifer/distilbart-cnn-12-6\" 모델\n",
        "3. \"human-centered-summarization/financial-summarization-pegasus\" 모델\n",
        "4. \"Bert-extractive-summarization\" 모델"
      ],
      "metadata": {
        "id": "xCcAIodh9vjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO-hJob59pIB",
        "outputId": "9effe5ff-9f93-4a04-833a-ec855c01614f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube_transcript_api) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2.10)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.7/dist-packages (12.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: rpunct in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langdetect==1.0.9 in /usr/local/lib/python3.7/dist-packages (from rpunct) (1.0.9)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from rpunct) (1.8.1)\n",
            "Requirement already satisfied: pandas==1.2.4 in /usr/local/lib/python3.7/dist-packages (from rpunct) (1.2.4)\n",
            "Requirement already satisfied: simpletransformers==0.61.4 in /usr/local/lib/python3.7/dist-packages (from rpunct) (0.61.4)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.7/dist-packages (from rpunct) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->rpunct) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->rpunct) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->rpunct) (1.21.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (1.4.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (1.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (2.23.0)\n",
            "Requirement already satisfied: transformers>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (4.17.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (2019.12.20)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (1.2.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (0.12.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (4.63.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (0.11.6)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.4->rpunct) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->rpunct) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (0.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (4.11.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (3.0.7)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (0.18.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (0.3.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (3.8.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.4->rpunct) (2022.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.4->rpunct) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.4->rpunct) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.4->rpunct) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.4->rpunct) (3.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.4->rpunct) (1.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (3.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.2.0->simpletransformers==0.61.4->rpunct) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers==0.61.4->rpunct) (3.1.0)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (1.4)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (3.1.27)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (0.7.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (7.1.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (2.13.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (4.2.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (0.8.1)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (2.1.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (3.17.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (1.5.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (0.18.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (5.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (0.10.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (2.1.6)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (4.2.4)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.4->rpunct) (1.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (2.11.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit->simpletransformers==0.61.4->rpunct) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit->simpletransformers==0.61.4->rpunct) (5.0.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (6.9.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (7.6.5)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (7.32.0)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.1.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (1.5.4)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (5.3.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (3.0.28)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (57.4.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.18.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (3.5.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers==0.61.4->rpunct) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (4.9.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.13.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.4->rpunct) (0.5.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (1.5.7)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (2.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (1.2.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (2.1.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers==0.61.4->rpunct) (1.0.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb->simpletransformers==0.61.4->rpunct) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리 설치\n",
        "\n",
        "!pip install youtube_transcript_api\n",
        "!pip install pytube\n",
        "!pip install transformers\n",
        "!pip install rpunct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from youtube_transcript_api import YouTubeTranscriptApi as yta\n",
        "from pytube import YouTube, extract\n",
        "from transformers import pipeline\n",
        "import re"
      ],
      "metadata": {
        "id": "1ao3Y24M-zvV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 테스트 영상\n",
        "1.   yh_subtitle_merge - \"https://www.youtube.com/watch?v=TYZoTlKN0rg\" : 3분 34초. U.S. economy is ‘not oil independent’: Yahoo Finance’s Rick Newman, Yahoo Finance\n",
        "2.   ted_subtitle_merge - \"https://www.youtube.com/watch?v=uXrCeiQxWyc\" : 18분 55초. What is economic value, and who creates it? | Mariana Mazzucato, TED\n",
        "3. gr_subtitle_merge - \"https://www.youtube.com/watch?v=3A19a8jpcDw\" : 8분 9초. Tutorial: Disable GOS (Game Optimizing Service) on Samsung Galaxy S22 Ultra, 60FPS Genshin Impact!, Golden Reviewer\n",
        "4. wion_subtitle_merge - \"https://www.youtube.com/watch?v=eT4LpF87aGk\" : 2분 54초. Ukraine war to lead to a frozen conflict? Russia's ploy to keep the West away | World English News, WION"
      ],
      "metadata": {
        "id": "Iu_fMplm__FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 영상 자막 추출\n",
        "- 유튜브 영상 링크로 자막 추출\n",
        "- 추출 후 길이와 앞부분 내용 확인"
      ],
      "metadata": {
        "id": "GmB6_uCw-nxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 링크 넣어서 자막 받아오는 함수 (돌려주는 건 합치지 않은 자막과 합친 자막)\n",
        "\n",
        "def get_subtitle(youtube_link) :\n",
        "  \n",
        "  video_id = extract.video_id(youtube_link)\n",
        "  subtitle = yta.get_transcript(video_id)\n",
        "  \n",
        "  subtitle_merge = ''\n",
        "  for line in subtitle :\n",
        "    subtitle_merge += line['text'] + \" \"\n",
        "  \n",
        "  return subtitle, subtitle_merge"
      ],
      "metadata": {
        "id": "q926aFu3_iWp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Yahoo"
      ],
      "metadata": {
        "id": "vLrAguMw_rHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 받아오고자 하는 유튜브 영상의 링크 넣기\n",
        "yh_subtitle, yh_subtitle_merge = get_subtitle(\"https://www.youtube.com/watch?v=TYZoTlKN0rg\")\n",
        "# 자막 길이 확인\n",
        "print(len(yh_subtitle_merge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oudd4qe__nsk",
        "outputId": "a9d84324-ed95-4364-93c8-66826eef3e4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 앞부분 확인\n",
        "print(yh_subtitle_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkRQIJoCBNn5",
        "outputId": "c2051eb4-e528-49a3-f681-bd5b41b06130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "as we watch the spike that we've been seeing in commodity prices we have heard a renewal of calls in the united states to drill more that the u.s should be energy independent our rick newman has been looking into that question um and it's it's not sort of as easy as that i think is the is the bottom line right rick there's a lot of confusion about this so i'm doing some reporting to try to bust some of these myths so people think the united states used to be energy independent so here's what that actually means when you combine all forms of energy that we produce in the united states that's oil but also natural gas also coal also renewables such as solar and wind yes we do consume more than we produce which i guess you could say makes us independent but we also participate in global markets which means we export a lot of that we still import energy and in terms of just oil we are not oil independent we have we still uh consume considerably more oil than we produce and we we have not been oil dependent since uh since the early days of uh the oil economy uh which goes all the way back to the 1860s so we have imported more oil than we have um produced for something like the last 45 or 50 years and that's probably going to continue and even though we we do need oil here in the united states a lot of u.s oil actually gets exported to other countries this is a global market um there's no way that the u.s president can put up walls so that all all the oil that's produced in the united states stays in the united states so that means we are susceptible to what happens in global oil markets we cannot fence ourselves off from what's going on in the rest of the world when it comes to oil prices hey rick you are a very much a forward thinker here gas prices record high i paid about over five dollars over the weekend for my premium unleaded that's just what my car takes that's what it recommends uh what do you think those will do or what do you think it means the midterm elections it's a great question and i'm not sure because uh ordinarily when gas prices go this high if they stay that high for any period of time it's just doom for the party that's in control but we saw some polls over the last few days with people americans saying i think more than 60 percent in one poll said it's worth paying more for gas prices as a way of uh tolerating the sanctions that are on russia and doing our part if you will to help out so there is a portion of voters who actually seem to be okay with this for the time being and we've also seen president biden's approval rating which bottomed out around 41 percent has actually ticked up lately so there does seem to be um some support for sort of uh rallying to the cause here i think it's going to be a different story if if gas prices stay this higher go higher for you know for the next eight months leading into the midterm elections because this is real pain for people uh you know who are dependent on cars and sazi i could have told you man um get a car that doesn't require premium fuel i it's serious how about one that just requires less fuel generally maybe get a hybrid or electric right coming around electric i mean i can't be ganged up here it's monday morning it's going to be a long week i think it i think it's time to comment i think it's no comment on wednesday keep it moving we're gonna we're gonna drag you into uh the 21st century whether you like it or not arthric newman thank you so much appreciate it \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Ted"
      ],
      "metadata": {
        "id": "RGASaV2fApZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 받아오고자 하는 유튜브 영상의 링크 넣기\n",
        "ted_subtitle, ted_subtitle_merge = get_subtitle(\"https://www.youtube.com/watch?v=uXrCeiQxWyc\")\n",
        "# 자막 길이 확인\n",
        "print(len(ted_subtitle_merge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0cH2hQaArr9",
        "outputId": "5792f12b-0fe1-4572-974d-97dde8e455ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 앞부분 확인\n",
        "print(ted_subtitle_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iljU4qKMBVIE",
        "outputId": "6bfe2ec8-80ee-4631-b164-95c9d18df723"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value creation. Wealth creation. These are really powerful words. Maybe you think of finance,\n",
            "you think of innovation, you think of creativity. But who are the value creators? If we use that word, we must be implying\n",
            "that some people aren't creating value. Who are they? The couch potatoes? The value extractors? The value destroyers? To answer this question, we actually\n",
            "have to have a proper theory of value. And I'm here as an economist\n",
            "to break it to you that we've kind of lost our way\n",
            "on this question. Now, don't look so surprised. What I mean by that is,\n",
            "we've stopped contesting it. We've stopped actually asking\n",
            "really tough questions about what is the difference between\n",
            "value creation and value extraction, productive and unproductive activities. Now, let me just give you\n",
            "some context here. 2009 was just about\n",
            "a year and a half after one of the biggest\n",
            "financial crises of our time, second only to the 1929 Great Depression, and the CEO of Goldman Sachs said Goldman Sachs workers are the most\n",
            "productive in the world. Productivity and productiveness,\n",
            "for an economist, actually has a lot to do with value. You're producing stuff, you're producing it\n",
            "dynamically and efficiently. You're also producing things\n",
            "that the world needs, wants and buys. Now, how this could have been said\n",
            "just one year after the crisis, which actually had this bank\n",
            "as well as many other banks -- I'm just kind of picking\n",
            "on Goldman Sachs here -- at the center of the crisis, because\n",
            "they had actually produced some pretty problematic financial products\n",
            "mainly but not only related to mortgages, which saw many thousands of people\n",
            "actually lose their homes. In 2010, in just one month, September, 120,000 people lost their homes\n",
            "through the foreclosures of that crisis. Between 2007 and 2010, 8.8 million people lost their jobs. The bank also had to then\n",
            "be bailed out by the US taxpayer for the sum of 10 billion dollars. We didn't hear the taxpayers bragging\n",
            "that they were value creators, but obviously, having bailed out one of the biggest value-creating\n",
            "productive companies, perhaps they should have. What I want to do next\n",
            "is kind of ask ourselves how we lost our way, how it could be, actually, that a statement like that\n",
            "could almost go unnoticed, because it wasn't an after-dinner joke;\n",
            "it was said very seriously. So what I want to do is bring you back\n",
            "300 years in economic thinking, when, actually, the term was contested. It doesn't mean that\n",
            "they were right or wrong, but you couldn't just call yourself\n",
            "a value creator, a wealth creator. There was a lot of debate\n",
            "within the economics profession. And what I want to argue is,\n",
            "we've kind of lost our way, and that has actually allowed this term,\n",
            "\"wealth creation\" and \"value,\" to become quite weak and lazy and also easily captured. OK? So let's start --\n",
            "I hate to break it to you -- 300 years ago. Now, what was interesting 300 years ago is the society was still\n",
            "an agricultural type of society. So it's not surprising\n",
            "that the economists of the time, who were called the Physiocrats, actually put the center\n",
            "of their attention to farm labor. When they said, \"Where\n",
            "does value come from?\" they looked at farming. And they produced what I think was\n",
            "probably the world's first spreadsheet, called the \"Tableau Economique,\" and this was done by François Quesnay,\n",
            "one of the leaders of this movement. And it was very interesting, because they didn't just say,\n",
            "\"Farming is the source of value.\" They then really worried about\n",
            "what was happening to that value when it was produced. What the Tableau Economique does -- and I've tried to make it\n",
            "a bit simpler here for you -- is it broke down the classes\n",
            "in society into three. The farmers, creating value,\n",
            "were called the \"productive class.\" Then others who were just\n",
            "moving some of this value around but it was useful, it was necessary, these were the merchants; they were called the \"proprietors.\" And then there was another class\n",
            "that was simply charging the farmers a fee for an existing asset, the land, and they called them the \"sterile class.\" Now, this is a really heavy-hitting word\n",
            "if you think what it means: that if too much of the resources\n",
            "are going to the landlords, you're actually putting the reproduction\n",
            "potential of the system at risk. And so all these little arrows there\n",
            "were their way of simulating -- again, spreadsheets and simulators,\n",
            "these guys were really using big data -- they were simulating what would\n",
            "actually happen under different scenarios if the wealth actually wasn't\n",
            "reinvested back into production to make that land more productive and was actually being\n",
            "siphoned out in different ways, or even if the proprietors\n",
            "were getting too much. And what later happened in the 1800s, and this was no longer\n",
            "the Agricultural Revolution but the Industrial Revolution, is that the classical economists, and these were Adam Smith, David Ricardo,\n",
            "Karl Marx, the revolutionary, also asked the question \"What is value?\" But it's not surprising that\n",
            "because they were actually living through an industrial era\n",
            "with the rise of machines and factories, they said it was industrial labor. So they had a labor theory of value. But again, their focus was reproduction, this real worry of what was happening\n",
            "to the value that was created if it was getting siphoned out. And in \"The Wealth of Nations,\" Adam Smith had this really great example\n",
            "of the pin factory where he said if you only have one person\n",
            "making every bit of the pin, at most you can make one pin a day. But if you actually invest in factory\n",
            "production and the division of labor, new thinking -- today, we would use the word\n",
            "\"organizational innovation\" -- then you could increase the productivity and the growth and the wealth of nations. So he showed that 10 specialized workers who had been invested in,\n",
            "in their human capital, could produce 4,800 pins a day, as opposed to just one\n",
            "by an unspecialized worker. And he and his fellow classical economists also broke down activities\n",
            "into productive and unproductive ones. (Laughter) And the unproductive ones weren't -- I think you're laughing because\n",
            "most of you are on that list, aren't you? (Laughter) Lawyers! I think he was right\n",
            "about the lawyers. Definitely not the professors,\n",
            "the letters of all kind people. So lawyers, professors,\n",
            "shopkeepers, musicians. He obviously hated the opera. He must have seen\n",
            "the worst performance of his life the night before writing this book. There's at least\n",
            "three professions up there that have to do with the opera. But this wasn't an exercise\n",
            "of saying, \"Don't do these things.\" It was just, \"What's going to happen if we actually end up allowing\n",
            "some parts of the economy to get too large without really thinking about\n",
            "how to increase the productivity of the source of the value\n",
            "that they thought was key, which was industrial labor. And again, don't ask yourself\n",
            "is this right or is this wrong, it was just very contested. By making these lists, it actually forced them also\n",
            "to ask interesting questions. And their focus,\n",
            "as the focus of the Physiocrats, was, in fact, on these objective\n",
            "conditions of production. They also looked, for example,\n",
            "at the class struggle. Their understanding of wages had to do with the objective,\n",
            "if you want, power relationships, the bargaining power of capital and labor. But again, factories, machines,\n",
            "division of labor, agricultural land\n",
            "and what was happening to it. So the big revolution\n",
            "that then happened -- and this, by the way, is not often\n",
            "taught in economics classes -- the big revolution that happened\n",
            "with the current system of economic thinking that we have, which is called \"neoclassical economics,\" was that the logic completely changed. It changed in two ways. It changed from this focus on\n",
            "objective conditions to subjective ones. Let me explain what I mean by that. Objective, in the way I just said. Subjective, in the sense that\n",
            "all the attention went to how individuals of different sorts\n",
            "make their decisions. OK, so workers are maximizing\n",
            "their choices of leisure versus work. Consumers are maximizing\n",
            "their so-called utility, which is a proxy for happiness, and firms are maximizing their profits. And the idea behind this was that\n",
            "then we can aggregate this up, and we see what that turns into, which are these nice, fancy\n",
            "supply-and-demand curves which produce a price, an equilibrium price. It's an equilibrium price,\n",
            "because we also added to it a lot of Newtonian physics equations where centers of gravity are very much\n",
            "part of the organizing principle. But the second point here is that\n",
            "that equilibrium price, or prices, reveal value. So the revolution here is a change\n",
            "from objective to subjective, but also the logic is no longer\n",
            "one of what is value, how is it being determined, what is the reproductive\n",
            "potential of the economy, which then leads to a theory of price but rather the reverse: a theory of price and exchange which reveals value. Now, this is a huge change. And it's not just an academic exercise,\n",
            "as fascinating as that might be. It affects how we measure growth. It affects how we steer economies\n",
            "to produce more of some activities, less of others, how we also remunerate\n",
            "some activities more than others. And it also just kind of makes you think, you know, are you happy to get out of bed\n",
            "if you're a value creator or not, and how is the price system itself\n",
            "if you aren't determining that? I mentioned it affects\n",
            "how we think about output. If we only include, for example, in GDP, those activities that have prices, all sorts of really weird things happen. Feminist economists\n",
            "and environmental economists have actually written\n",
            "about this quite a bit. Let me give you some examples. If you marry your babysitter,\n",
            "GDP will go down, so do not do it. Do not be tempted to do this, OK? Because an activity that perhaps was\n",
            "before being paid for is still being done but is no longer paid. (Laughter) If you pollute, GDP goes up. Still don't do it, but if you do it,\n",
            "you'll help the economy. Why? Because we have to actually\n",
            "pay someone to clean it. Now, what's also really interesting\n",
            "is what happened to finance in the financial sector in GDP. This also, by the way, is something\n",
            "I'm always surprised that many economists don't know. Up until 1970, most of the financial sector\n",
            "was not even included in GDP. It was kind of indirectly,\n",
            "perhaps not knowingly, still being seen through the eyes\n",
            "of the Physiocrats as just kind of moving stuff around,\n",
            "not actually producing anything new. So only those activities\n",
            "that had an explicit price were included. For example, if you went to get\n",
            "a mortgage, you were charged a fee. That went into GDP and the national\n",
            "income and product accounting. But, for example,\n",
            "net interest payments didn't, the difference between\n",
            "what banks were earning in interest if they gave you a loan and what\n",
            "they were paying out for a deposit. That wasn't being included. And so the people doing the accounting\n",
            "started to look at some data, which started to show\n",
            "that the size of finance and these net interest payments were actually growing substantially. And they called this\n",
            "the \"banking problem.\" These were some people working\n",
            "inside, actually, the United Nations in a group called the Systems\n",
            "of National [Accounts], SNA. They called it the \"banking problem,\" like, \"Oh my God, this thing is huge,\n",
            "and we're not even including it.\" So instead of stopping and actually\n",
            "making that Tableau Economique or asking some of these\n",
            "fundamental questions that also the classicals were asking\n",
            "about what is actually happening, the division of labor between different\n",
            "types of activities in the economy, they simply gave these\n",
            "net interest payments a name. So the commercial banks, they called this\n",
            "\"financial intermediation.\" That went into the NIPA accounts. And the investment banks\n",
            "were called the \"risk-taking activities,\" and that went in. In case I haven't explained this properly, that red line is showing how much quicker financial intermediation\n",
            "as a whole was growing compared to the rest of the economy,\n",
            "the blue line, industry. And so this was quite extraordinary, because what actually happened,\n",
            "and what we know today, and there's different people\n",
            "writing about this, this data here\n",
            "is from the Bank of England, is that lots of what finance\n",
            "was actually doing from the 1970s and '80s on was basically financing itself: finance financing finance. And what I mean by that is finance,\n",
            "insurance and real estate. In fact, in the UK, something like between\n",
            "10 and 20 percent of finance finds its way into\n",
            "the real economy, into industry, say, into the energy sector,\n",
            "into pharmaceuticals, into the IT sector, but most of it goes back\n",
            "into that acronym, FIRE: finance, insurance and real estate. It's very conveniently called FIRE. Now, this is interesting because, in fact, it's not to say that finance\n",
            "is good or bad, but the degree to which, by just having to give it a name, because it actually had an income\n",
            "that was being generated, as opposed to pausing and asking,\n",
            "\"What is it actually doing?\" -- that was a missed opportunity. Similarly, in the real economy,\n",
            "in industry itself, what was happening? And this real focus on prices\n",
            "and also share prices has created a huge problem\n",
            "of reinvestment, again, this real attention that both\n",
            "the Physiocrats and the classicals had to the degree to which the value\n",
            "that was being generated in the economy was in fact being reinvested back in. And so what we have today is\n",
            "an ultrafinancialized industrial sector where, increasingly, a share\n",
            "of the profits and the net income are not actually going\n",
            "back into production, into human capital training,\n",
            "into research and development but just being siphoned out\n",
            "in terms of buying back your own shares, which boosts stock options,\n",
            "which is, in fact, the way that many executives are getting paid. And, you know, some\n",
            "share buybacks is absolutely fine, but this system\n",
            "is completely out of whack. These numbers that I'm showing you here show that in the last 10 years,\n",
            "466 of the S and P 500 companies have spent over four trillion\n",
            "on just buying back their shares. And what you see then if you aggregate\n",
            "this up at the macroeconomic level, so if we look at aggregate\n",
            "business investment, which is a percentage of GDP, you also see this falling level\n",
            "of business investment. And this is a problem. This, by the way, is a huge problem\n",
            "for skills and job creation. You might have heard there's lots\n",
            "of attention these days to, \"Are the robots taking our jobs?\" Well, mechanization has\n",
            "for centuries, actually, taken jobs, but as long as profits were being\n",
            "reinvested back into production, then it didn't matter: new jobs appeared. But this lack of reinvestment\n",
            "is, in fact, very dangerous. Similarly, in the pharmaceutical industry,\n",
            "for example, how prices are set, it's quite interesting how it doesn't look\n",
            "at these objective conditions of the collective way in which value\n",
            "is created in the economy. So in the sector where you have\n",
            "lots of different actors -- public, private, of course, but also\n",
            "third-sector organizations -- creating value, the way we actually measure\n",
            "value in this sector is through the price system itself. Prices reveal value. So when, recently, the price of an antibiotic\n",
            "went up by 400 percent overnight, and the CEO was asked,\n",
            "\"How can you do this? People actually need that antibiotic. That's unfair.\" He said, \"Well, we have a moral imperative to allow prices to go\n",
            "what the market will bear,\" completely dismissing the fact\n",
            "that in the US, for example, the National Institutes of Health\n",
            "spent over 30 billion a year on the medical research\n",
            "that actually leads to these drugs. So, again, a lack of attention\n",
            "to those objective conditions and just allowing the price system\n",
            "itself to reveal the value. Now, this is not just\n",
            "an academic exercise, as interesting as it may be. All this really matters\n",
            "[for] how we measure output, to how we steer the economy, to whether you feel\n",
            "that you're productive, to which sectors we end up\n",
            "helping, supporting and also making people feel\n",
            "proud to be part of. In fact, going back to that quote, it's not surprising that Blankfein\n",
            "could say that. He was right. In the way that we actually measure\n",
            "production, productivity and value in the economy, of course Goldman Sachs workers\n",
            "are the most productive. They are in fact earning the most. The price of their labor\n",
            "is revealing their value. But this becomes tautological, of course. And so there's a real need to rethink. We need to rethink\n",
            "how we're measuring output, and in fact there's some\n",
            "amazing experiments worldwide. In New Zealand, for example, they now have\n",
            "a gross national happiness indicator. In Bhutan, also, they're thinking\n",
            "about happiness and well-being indicators. But the problem is that we can't\n",
            "just be adding things in. We do have to pause, and I think this should be\n",
            "a moment for pause, given that we see so little\n",
            "has actually changed since the financial crisis, to make sure that\n",
            "we are not also confusing value extraction with value creation, so looking at what's included,\n",
            "not just adding more, to make sure that we're not, for example,\n",
            "confusing rents with profits. Rents for the classicals\n",
            "was about unearned income. Today, rents, when they're\n",
            "talked about in economics, is just an imperfection\n",
            "towards a competitive price that could be competed away\n",
            "if you take away some asymmetries. Second, we of course can steer\n",
            "activities into what the classicals called the \"production boundary.\" This should not be an us-versus-them, big, bad finance versus\n",
            "good, other sectors. We could reform finance. There was a real lost opportunity\n",
            "in some ways after the crisis. We could have had\n",
            "the financial transaction tax, which would have rewarded\n",
            "long-termism over short-termism, but we didn't decide to do that globally. We can. We can change our minds. We can also set up\n",
            "new types of institutions. There's different types of, for example,\n",
            "public financial institutions worldwide that are actually providing that patient,\n",
            "long-term, committed finance that helps small firms grow, that help\n",
            "infrastructure and innovation happen. But this shouldn't just be about output. This shouldn't just be about\n",
            "the rate of output. We should also as a society pause and ask: What value are we even creating? And I just want to end with the fact\n",
            "that this week we are celebrating the 50th anniversary of the Moon landing. This required the public sector,\n",
            "the private sector, to invest and innovate\n",
            "in all sorts of ways, not just around aeronautics. It included investment in areas\n",
            "like nutrition and materials. There were lots of actual mistakes\n",
            "that were done along the way. In fact, what government did was it used\n",
            "its full power of procurement, for example, to fuel\n",
            "those bottom-up solutions, of which some failed. But are failures part of value creation? Or are they just mistakes? Or how do we actually also\n",
            "nurture the experimentation, the trial and error and error and error? Bell Labs, which was\n",
            "the R and D laboratory of AT and T, actually came from an era\n",
            "where government was quite courageous. It actually asked AT and T that\n",
            "in order to maintain its monopoly status, it had to reinvest its profits\n",
            "back into the real economy, innovation and innovation beyond telecoms. That was the history,\n",
            "the early history of Bell Labs. So how we can get these new conditions\n",
            "around reinvestment to collectively invest\n",
            "in new types of value directed at some of the biggest\n",
            "challenges of our time, like climate change? This is a key question. But we should also ask ourselves, had there been a net\n",
            "present value calculation or a cost-benefit analysis done about whether or not to even try\n",
            "to go to the Moon and back again in a generation, we probably wouldn't have started. So thank God, because I'm an economist,\n",
            "and I can tell you, value is not just price. Thank you. (Applause) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) Golden Review"
      ],
      "metadata": {
        "id": "VgLgYvTRAr8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 받아오고자 하는 유튜브 영상의 링크 넣기\n",
        "gr_subtitle, gr_subtitle_merge = get_subtitle(\"https://www.youtube.com/watch?v=3A19a8jpcDw\")\n",
        "# 자막 길이 확인\n",
        "print(len(gr_subtitle_merge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9xGX91aAu78",
        "outputId": "e0c8c22a-f158-4180-842f-f900b90fdb50"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 앞부분 확인\n",
        "print(gr_subtitle_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp2ID7CIBcEK",
        "outputId": "89d721ea-f337-40c8-f97a-1dc4318cac65"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi guys welcome back to golden reviewer today i'm going to show you how to disable game optimization service on samsung smartphone first you search for this netgrad app in play store and install it then you open the app go to settings advanced options and select manage system apps because the apps we are going to block are all system apps then we search for the keyword game there are three apps we want to disable one is game optimization service another is game booster and game launcher as well for game booster plus and game plugins i think they they don't really matter but to be safe here i just uninstall them because they are not system apps so uh you can just uninstall them but for the three apps we are going to block their system apps so just click these two buttons here to block them from accessing the internet and then you press the button on the top left corner to make sure that netgard is enabled then you just accept all the promotes and until you see this key in your status bar that means it's running the next step we are going to do is we go to the settings of these apps you can actually directly go there from within the netguard which is very convenient go to storage and clear data for all three apps the rationale is that the game optimization service actually download a full list of app signatures from samsung server when it's running in the background and it will throttle according to this list of app names so we block them from accessing the internet so they cannot download the app name list and then we clear the app data to remove any already downloaded app name list so that the game optimization service does not know which apps to throttle and finally turn off your wi-fi or data and ending and restart your phone this is because we want to make sure we run the netgard app before the game optimization service can auto start from background uh when the device first boots up right so turn off your internet so even the game optimization service put up it won't be able to download anything and the first thing we're going to do is to turn on the netgraph to make sure that they are blocked and then we can turn on the wi-fi here you can see i just switch on the toggle again just to make sure and then you will be able to see this notification in your notification panel right that means the netguard is already working to block the internet access and now you can turn on your wi-fi and you can continue to play your games without game optimization service and next i'll show you how much a difference it makes to actually turn on and off the game optimization service okay so here is my galaxy s22 ultra it's the snapdragon version of course i'll test the exynos version later on in another video maybe but here this is just to show you how much a difference the game optimization service actually makes right so the one on the left is which game optimization service on and uh one on the right is which is off okay so first thing you notice is how much smoother the one on the right is and this is confirmed by the fps reading as well we are seeing something around 50 just now at the beginning we even have 60 on the other hand on the left we can see if we leave game optimization series on the fps is only around 30 to 40 maybe at most it can reach 40 something but most of the time is from 30 to 40 right this also was confirmed by our previous test where if you play for 10 minutes the average fps is only around 32 so there's a clear difference in terms of frame rate but we also see another difference in power consumption if you look at the power consumption on the right it's much higher than the one on the left okay so higher performance requires higher power consumption and this will result in more heat as well okay so turning off game optimization service does not magically make your device uh just better in every way right yes it can run the game smoother but it will generate more heat and it will use more batching and in the end if your device becomes too hot thermal throttling will still keep it and it will still restrict your performance and your fps will become lower as well actually we are seeing that chart here right you will notice that uh the one on the right has started to heat up and the throttle the fps starts to drop i mean it's still higher than the one on the left with gos on but that's physics you cannot change it your device heats up you have to throttle otherwise it will eventually melt or explode right but then this gives us more possibility if you use a cooler or if you are just staying at a cooler place and i think in that case your game will run much smoother and that was not possible with gos on and here we can use this cpu float to understand this issue more clearly if you look at the cpu gpu frequency you can see that with gos on the cpu is restricted to a very low frequency no matter what you do the frequency won't go up and it's the same story for gpu with gos on the gpu frequency actually makes it out at around 300 megahertz while with gos off it can actually reach around 500 megahertz right and then the power consumption is very different as well with gos on it's own only about 1200 ma but with gos off we see that the current can go up to 2000 milliampere right so lastly we can see the performance comparison the fps record with gos on and off we can see that's a very clear difference with gos off we get more than 10 fps more and the game indeed run much smoother and then for power we see that the power consumption is also much higher right we see an overall more than 50 percent power consumption increase and especially for the first two minutes when the device has not started to throttle um the average power may be even more than eight watts which is pretty high and we see this gradual throttling pattern as we see from other devices so but if we have gos on the power is restricted to a very low level even from the beginning so we don't see any change in power consumption there is no throttling at all because it started with a low performance alright guys so that's all for today's video and i have to say that i took the inspiration from a korean channel and i'll leave the link down below if you understand korean you can go check out so i think he actually invented this blocking the internet access method so i just uh took it and i use um maybe a better free software to do this and to be honest this method for now is still pretty tedious and it's not really that practical but of course i'll try to do more tests on this and we are all waiting for the system update from samsung who actually promised to fix the issue all right so that's all for today thanks for watching and see you next time \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) WION"
      ],
      "metadata": {
        "id": "pHA4H_C_AvSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 받아오고자 하는 유튜브 영상의 링크 넣기\n",
        "wion_subtitle, wion_subtitle_merge = get_subtitle(\"https://www.youtube.com/watch?v=eT4LpF87aGk\")\n",
        "# 자막 길이 확인\n",
        "print(len(wion_subtitle_merge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RGkQizNAwm4",
        "outputId": "f7805267-40a3-4aba-c803-64e38878b221"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자막 앞부분 확인\n",
        "print(wion_subtitle_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hizalr8Be3m",
        "outputId": "a85f83b3-c5da-4a5c-b52f-f6712e267528"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the russia-ukraine war could be headed for a frozen conflict the situation on ground is evolving in such a manner that a frozen conflict is more likely the way forward to end the war experts say that humanitarian sees fires hinting towards that possibility as well but first let's tell you what a frozen conflict is a frozen conflict is a situation in which active armed conflict has ended but without a formal peace agreement or other political framework in place frozen conflict occurs in regions of a country no longer controlled by its central authorities these zones then remain under jurisdiction of the separators as a result states backing the separators run their puppet governments moreover the lack of non-violent solutions failed to permanently end conflict this form of conflict was unique to a few former soviet republics especially during the collapse of the soviet union in 1991. now russia is often accused of destabilizing its former soviet neighbors to keep them in its sphere of influence and stop them from integrating with the nato and eu that is why russia annex crimea destabilized eastern ukraine in 2014 and days before russia invaded ukraine it recognized the two separatist regions donesk and luhansk on the 21st of february moldova's breakaway territory and georgia's rebel regions of south ossetia and abkhazia are also examples of this policy by russia more recent is the example of nagorno-karabakh a disputed region between armenians and the azeri's in 2020 a ceasefire agreed that azerbaijan would regain the territories it lost earlier and gained during its military maneuvers in 2020 but in georgia moldova azerbaijan now donbass in ukraine moscow is known to have favored separatist sentiments in all of these regions this is because the kremlin aims to create a conflict without a solution only so that it can step in to keep peace on its own terms while many may believe that the war in ukraine has just begun and there is still some time for a frozen conflict the recent humanitarian ceasefires in ukraine indicate a strong possibility of it hello beyond is now available in your country download the app now get all the news on the move \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 영상 별 추출 자막 변수\n",
        "1.   yh_subtitle_merge - 자막 길이 : 3,491\n",
        "2.   ted_subtitle_merge - 자막 길이 : 19,767\n",
        "3. gr_subtitle_merge - 자막 길이 : 6,735\n",
        "4. wion_subtitle_merge - 자막 길이 : 2,178"
      ],
      "metadata": {
        "id": "eFKWXS5AAkQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 전처리 자막 생성\n",
        "1.  괄호나 대괄호로 처리된 description이 있는 경우 제거   \n",
        " ex. [Music]\n",
        "2. 구어에서 발생하는 filler words 제거 : 만들어진 filler words 리스트가 없어서 직접 구글 검색 등을 통해 사용 빈도가 높은 filler words 들을 수집하여 list를 생성함\n",
        "3. 반복단어 제거\n",
        "4. 문장 구별 및 끝점 추가(rpunct) : https://huggingface.co/felflare/bert-restore-punctuation?text=My+name+is+Clara+and+I+live+in+Berkeley%2C+California."
      ],
      "metadata": {
        "id": "R6bfYFuVBz6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 수집한 fillerwords 목록을 fillerwords 변수에 할당\n",
        "fillerwords = [\"i mean\", \"basically\", \"you know\", \"umm\", \"um\", \"uh\", \"huh\", \"er\", \"eh\", \"ah\", \"like that\", \"just\", \"really\", \"somehow\", \"i guess\", \"i suppose\", \"like i said\", \"or something like that\", \"kind of\", \"sort of\", \"you see\", \"see what i mean\", \"yeah\"]"
      ],
      "metadata": {
        "id": "I5SimYlWCLpx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fw_list = []\n",
        "for i in fillerwords :\n",
        "  fw_list.append(\" \" + i + \" \")\n",
        "fw_list = '|'.join(fw_list)\n",
        "fw_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9nrm6bOGCOY_",
        "outputId": "e57e6e49-6a84-4432-f74f-ecffb14eb14c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' i mean | basically | you know | umm | um | uh | huh | er | eh | ah | like that | just | really | somehow | i guess | i suppose | like i said | or something like that | kind of | sort of | you see | see what i mean | yeah '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 구별 및 끝점 추가(rpunct) 라이브러리\n",
        "from rpunct import RestorePuncts"
      ],
      "metadata": {
        "id": "YuN85ST1Ck4I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Yahoo\n",
        "\n",
        "*   전처리 이전 : yh_subtitle_merge : 3,491\n",
        "*   전처리 이후 : yh_cleaned_text : 3,458"
      ],
      "metadata": {
        "id": "ZM2rFqGRBwuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일단 자막 space로 split하고, '['나 '('가 포함되어 있는건 삭제, 아니면(보통 문자열이면) cleaned 된 문자열 리스트에 추가 \n",
        "\n",
        "splited = yh_subtitle_merge.split()\n",
        "yh_bracket_cleaned_subtitle = \"\"\n",
        "for i in splited :\n",
        "  if '[' in i or '(' in i :\n",
        "    pass\n",
        "  else :\n",
        "    yh_bracket_cleaned_subtitle += (i + ' ')\n",
        "\n",
        "len(yh_bracket_cleaned_subtitle)\n",
        "\n",
        "# !!!!!!! 현재는 ()나 []안에 단어가 하나일 때는 잘 제거되지만 만약 그 안에 서술이 공백을 포함해 두 단어 이상 있으면 제대로 제거되지 않음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX1VD-M5Bmsd",
        "outputId": "24661c58-f49c-49fe-8e4b-f5a61841491e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3491"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fw_cleaned_test에 클린이 된 text가 할당됨\n",
        "yh_fw_cleaned_test = re.sub(fw_list, \" \", yh_bracket_cleaned_subtitle)\n",
        "yh_fw_cleaned_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Ys9UsYeSCRdJ",
        "outputId": "89f390f6-9e54-434e-cac4-9e607e28e5f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"as we watch the spike that we've been seeing in commodity prices we have heard a renewal of calls in the united states to drill more that the u.s should be energy independent our rick newman has been looking into that question and it's it's not as easy as that i think is the is the bottom line right rick there's a lot of confusion about this so i'm doing some reporting to try to bust some of these myths so people think the united states used to be energy independent so here's what that actually means when you combine all forms of energy that we produce in the united states that's oil but also natural gas also coal also renewables such as solar and wind yes we do consume more than we produce which you could say makes us independent but we also participate in global markets which means we export a lot of that we still import energy and in terms of oil we are not oil independent we have we still consume considerably more oil than we produce and we we have not been oil dependent since since the early days of the oil economy which goes all the way back to the 1860s so we have imported more oil than we have produced for something like the last 45 or 50 years and that's probably going to continue and even though we we do need oil here in the united states a lot of u.s oil actually gets exported to other countries this is a global market there's no way that the u.s president can put up walls so that all all the oil that's produced in the united states stays in the united states so that means we are susceptible to what happens in global oil markets we cannot fence ourselves off from what's going on in the rest of the world when it comes to oil prices hey rick you are a very much a forward thinker here gas prices record high i paid about over five dollars over the weekend for my premium unleaded that's what my car takes that's what it recommends what do you think those will do or what do you think it means the midterm elections it's a great question and i'm not sure because ordinarily when gas prices go this high if they stay that high for any period of time it's doom for the party that's in control but we saw some polls over the last few days with people americans saying i think more than 60 percent in one poll said it's worth paying more for gas prices as a way of tolerating the sanctions that are on russia and doing our part if you will to help out so there is a portion of voters who actually seem to be okay with this for the time being and we've also seen president biden's approval rating which bottomed out around 41 percent has actually ticked up lately so there does seem to be some support for uh rallying to the cause here i think it's going to be a different story if if gas prices stay this higher go higher for for the next eight months leading into the midterm elections because this is real pain for people you know who are dependent on cars and sazi i could have told you man get a car that doesn't require premium fuel i it's serious how about one that requires less fuel generally maybe get a hybrid or electric right coming around electric i can't be ganged up here it's monday morning it's going to be a long week i think it i think it's time to comment i think it's no comment on wednesday keep it moving we're gonna we're gonna drag you into the 21st century whether you like it or not arthric newman thank you so much appreciate it \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rpunct를 사용해 문장 구별 및 온점 처리\n",
        "rpunct = RestorePuncts()\n",
        "yh_cleaned_text = rpunct.punctuate(yh_fw_cleaned_test)\n",
        "\n",
        "# 전처리 후 문장 길이 및 앞부분 확인\n",
        "print(len(yh_cleaned_text))\n",
        "print(yh_cleaned_text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "a39fcbc79ab04231b41ef4481d344db4",
            "43dcc835a7db4efe9b3a87790bfc08d9",
            "f32d6f1bdd2f44c0ab831c4568ceab50",
            "cdb02bc9896c4def9834342fbeab071e",
            "bbdaaf6dd1f84f41b9c1068809ca1991",
            "a622b121b41246b182407b4e30bd98cd",
            "877e54bb51544ec398bc4019e4551e6d",
            "39b9041148d5455ab9293e8f0283bbcd",
            "3cd5524361b544c1a05466fccf7887b8",
            "3e9fdb7577894f2cbd2fb61fa7249d76",
            "7903353e2b9c43718c6fe9341acfa405",
            "c73433063e7c41fdb0fd3d0061e58209",
            "39eb4ebf664342cc800cd8a2324738c7",
            "07552a07f1f7440da8abecb08f88f902",
            "f031eb2ba23d4a29b6b8bc9f2d61b3b6",
            "9c97cef06644416aa64bba7f4ad0ef64",
            "14574c1addec48cf9da45ebd1b3fa217",
            "40fddec9bd3748b6b61b1c40130b93c6",
            "18ea2a1411ed4f548edd50d1f3cd6356",
            "e1688fb8d9da4bc981ec8ff9343db7f9",
            "c14e62d063704a48bae08f222644942d",
            "478e4042719b4f0184630af6ddc10295",
            "d74ba00ecc9d4aaf9efaa39f292b4108",
            "855227810065474a8fb657eda383867f",
            "72b817c09afc4d679fc83d220e27c740",
            "9df89c6b76ea4a34827e7cd061b55ec0",
            "645218a39da44828bcd7de76e48f85a7",
            "0105fc0d73334c33a067ec3f500837ba",
            "a99b628059dc4eda9f09614f501f2018",
            "0fe4c22fb3394d93ab1190815ecec19c",
            "8ef27b825988423daf3607a9ba8f847b",
            "025027abaa6a4113910f3a9f8599c796",
            "1164f20809c14009892ded7b79b5467f",
            "2f9e360354e64a1fae1e68ea062a7b18",
            "39882f204c894d91b9f3d430ae21bd79",
            "660612611caa426e8f9b25651c2333c1",
            "cd0ba90d56ec4af9b0f3b055c5216f10",
            "65f941e684f34b96b7519696cb8c20b0",
            "d367df6a0c804b51911a23a954fefc75",
            "eb2befec138b45098681f11713c5adba",
            "a02127d790ea4b75b7002fb303e5176a",
            "6f32ceadb336433ab762fd0d90daec81",
            "f724ab0ec54b45f39a14a3b87c5f82e3",
            "4b0f8319351641ed88fc9798e8f00b53",
            "9897889b41114c97bd661e19058705b8",
            "c71335cc762848cdaa6b2c8ca7357162",
            "a0c800df3ebf4483b9fe15ad41ecc80f",
            "df87cfa898394818a2858050ee5cd06b",
            "45bac38fada242ceb7e0336f16a16ca8",
            "3ce0a956dc8d4c98b4cd3d2813e34f41",
            "9c100b132a3f4b758047d47dded74e5c",
            "4f5d28f875504775b4d5c8e186d84120",
            "36bd7be61e0d472eb6d9a564e266e2d1",
            "df8aeed6d2d04a338ee2f40e59aa1af3",
            "0a7ac03edd8c48288048184f246994a1"
          ]
        },
        "id": "fOaWdBpgCwpw",
        "outputId": "30b42306-cdc0-4da1-b04b-77b4c7db8452"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a39fcbc79ab04231b41ef4481d344db4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c73433063e7c41fdb0fd3d0061e58209"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d74ba00ecc9d4aaf9efaa39f292b4108"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f9e360354e64a1fae1e68ea062a7b18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/530 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9897889b41114c97bd661e19058705b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3458\n",
            "As we watch the spike that we've been seeing in commodity prices, we have heard a renewal of calls in the United States to drill more that the U.s should be energy independent. Our Rick Newman has been looking into that question and it's It's not as easy as that I think is the is the bottom line, right Rick? There's a lot of confusion about this, so I'm doing some reporting to try to bust some of these myths. So people think the United States used to be energy independent. So here's what that actually means when you combine all forms of energy that we produce in the United States, that's oil, but also natural gas. also coal. also renewables such as solar and wind. Yes, we do consume more than we produce, which you could say makes us independent. but we also participate in global markets, which means we export a lot of that. We still import energy, and in terms of oil, we are not oil independent. We have. We still consume considerably more oil than we produce, and we we have not been oi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) TED\n",
        "\n",
        "*   전처리 이전 : ted_subtitle_merge : 19,767\n",
        "*   전처리 이후 : ted_cleaned_text : 19,519"
      ],
      "metadata": {
        "id": "Ew8aHrL_DBk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일단 자막 space로 split하고, '['나 '('가 포함되어 있는건 삭제, 아니면(보통 문자열이면) cleaned 된 문자열 리스트에 추가 \n",
        "\n",
        "splited = ted_subtitle_merge.split()\n",
        "ted_bracket_cleaned_subtitle = \"\"\n",
        "for i in splited :\n",
        "  if '[' in i or '(' in i :\n",
        "    pass\n",
        "  else :\n",
        "    ted_bracket_cleaned_subtitle += (i + ' ')\n",
        "\n",
        "len(ted_bracket_cleaned_subtitle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RqiBFmUDFv_",
        "outputId": "4657c432-d60d-46b1-f130-e611860e41af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19705"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fw_cleaned_test에 클린이 된 text가 할당됨\n",
        "ted_fw_cleaned_test = re.sub(fw_list, \" \", ted_bracket_cleaned_subtitle)\n",
        "ted_fw_cleaned_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "HfQ7Dxp5DKBx",
        "outputId": "5ebbe037-0e7c-4385-9732-2d957d2c38c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Value creation. Wealth creation. These are powerful words. Maybe you think of finance, you think of innovation, you think of creativity. But who are the value creators? If we use that word, we must be implying that some people aren\\'t creating value. Who are they? The couch potatoes? The value extractors? The value destroyers? To answer this question, we actually have to have a proper theory of value. And I\\'m here as an economist to break it to you that we\\'ve lost our way on this question. Now, don\\'t look so surprised. What I mean by that is, we\\'ve stopped contesting it. We\\'ve stopped actually asking tough questions about what is the difference between value creation and value extraction, productive and unproductive activities. Now, let me give you some context here. 2009 was about a year and a half after one of the biggest financial crises of our time, second only to the 1929 Great Depression, and the CEO of Goldman Sachs said Goldman Sachs workers are the most productive in the world. Productivity and productiveness, for an economist, actually has a lot to do with value. You\\'re producing stuff, you\\'re producing it dynamically and efficiently. You\\'re also producing things that the world needs, wants and buys. Now, how this could have been said one year after the crisis, which actually had this bank as well as many other banks -- I\\'m kind of picking on Goldman Sachs here -- at the center of the crisis, because they had actually produced some pretty problematic financial products mainly but not only related to mortgages, which saw many thousands of people actually lose their homes. In 2010, in one month, September, 120,000 people lost their homes through the foreclosures of that crisis. Between 2007 and 2010, 8.8 million people lost their jobs. The bank also had to then be bailed out by the US taxpayer for the sum of 10 billion dollars. We didn\\'t hear the taxpayers bragging that they were value creators, but obviously, having bailed out one of the biggest value-creating productive companies, perhaps they should have. What I want to do next is ask ourselves how we lost our way, how it could be, actually, that a statement could almost go unnoticed, because it wasn\\'t an after-dinner joke; it was said very seriously. So what I want to do is bring you back 300 years in economic thinking, when, actually, the term was contested. It doesn\\'t mean that they were right or wrong, but you couldn\\'t call yourself a value creator, a wealth creator. There was a lot of debate within the economics profession. And what I want to argue is, we\\'ve lost our way, and that has actually allowed this term, \"wealth creation\" and \"value,\" to become quite weak and lazy and also easily captured. OK? So let\\'s start -- I hate to break it to you -- 300 years ago. Now, what was interesting 300 years ago is the society was still an agricultural type of society. So it\\'s not surprising that the economists of the time, who were called the Physiocrats, actually put the center of their attention to farm labor. When they said, \"Where does value come from?\" they looked at farming. And they produced what I think was probably the world\\'s first spreadsheet, called the \"Tableau Economique,\" and this was done by François Quesnay, one of the leaders of this movement. And it was very interesting, because they didn\\'t say, \"Farming is the source of value.\" They then worried about what was happening to that value when it was produced. What the Tableau Economique does -- and I\\'ve tried to make it a bit simpler here for you -- is it broke down the classes in society into three. The farmers, creating value, were called the \"productive class.\" Then others who were moving some of this value around but it was useful, it was necessary, these were the merchants; they were called the \"proprietors.\" And then there was another class that was simply charging the farmers a fee for an existing asset, the land, and they called them the \"sterile class.\" Now, this is a heavy-hitting word if you think what it means: that if too much of the resources are going to the landlords, you\\'re actually putting the reproduction potential of the system at risk. And so all these little arrows there were their way of simulating -- again, spreadsheets and simulators, these guys were using big data -- they were simulating what would actually happen under different scenarios if the wealth actually wasn\\'t reinvested back into production to make that land more productive and was actually being siphoned out in different ways, or even if the proprietors were getting too much. And what later happened in the 1800s, and this was no longer the Agricultural Revolution but the Industrial Revolution, is that the classical economists, and these were Adam Smith, David Ricardo, Karl Marx, the revolutionary, also asked the question \"What is value?\" But it\\'s not surprising that because they were actually living through an industrial era with the rise of machines and factories, they said it was industrial labor. So they had a labor theory of value. But again, their focus was reproduction, this real worry of what was happening to the value that was created if it was getting siphoned out. And in \"The Wealth of Nations,\" Adam Smith had this great example of the pin factory where he said if you only have one person making every bit of the pin, at most you can make one pin a day. But if you actually invest in factory production and the division of labor, new thinking -- today, we would use the word \"organizational innovation\" -- then you could increase the productivity and the growth and the wealth of nations. So he showed that 10 specialized workers who had been invested in, in their human capital, could produce 4,800 pins a day, as opposed to one by an unspecialized worker. And he and his fellow classical economists also broke down activities into productive and unproductive ones. And the unproductive ones weren\\'t -- I think you\\'re laughing because most of you are on that list, aren\\'t you? Lawyers! I think he was right about the lawyers. Definitely not the professors, the letters of all kind people. So lawyers, professors, shopkeepers, musicians. He obviously hated the opera. He must have seen the worst performance of his life the night before writing this book. There\\'s at least three professions up there that have to do with the opera. But this wasn\\'t an exercise of saying, \"Don\\'t do these things.\" It was just, \"What\\'s going to happen if we actually end up allowing some parts of the economy to get too large without thinking about how to increase the productivity of the source of the value that they thought was key, which was industrial labor. And again, don\\'t ask yourself is this right or is this wrong, it was very contested. By making these lists, it actually forced them also to ask interesting questions. And their focus, as the focus of the Physiocrats, was, in fact, on these objective conditions of production. They also looked, for example, at the class struggle. Their understanding of wages had to do with the objective, if you want, power relationships, the bargaining power of capital and labor. But again, factories, machines, division of labor, agricultural land and what was happening to it. So the big revolution that then happened -- and this, by the way, is not often taught in economics classes -- the big revolution that happened with the current system of economic thinking that we have, which is called \"neoclassical economics,\" was that the logic completely changed. It changed in two ways. It changed from this focus on objective conditions to subjective ones. Let me explain what I mean by that. Objective, in the way I said. Subjective, in the sense that all the attention went to how individuals of different sorts make their decisions. OK, so workers are maximizing their choices of leisure versus work. Consumers are maximizing their so-called utility, which is a proxy for happiness, and firms are maximizing their profits. And the idea behind this was that then we can aggregate this up, and we see what that turns into, which are these nice, fancy supply-and-demand curves which produce a price, an equilibrium price. It\\'s an equilibrium price, because we also added to it a lot of Newtonian physics equations where centers of gravity are very much part of the organizing principle. But the second point here is that that equilibrium price, or prices, reveal value. So the revolution here is a change from objective to subjective, but also the logic is no longer one of what is value, how is it being determined, what is the reproductive potential of the economy, which then leads to a theory of price but rather the reverse: a theory of price and exchange which reveals value. Now, this is a huge change. And it\\'s not an academic exercise, as fascinating as that might be. It affects how we measure growth. It affects how we steer economies to produce more of some activities, less of others, how we also remunerate some activities more than others. And it also kind of makes you think, you know, are you happy to get out of bed if you\\'re a value creator or not, and how is the price system itself if you aren\\'t determining that? I mentioned it affects how we think about output. If we only include, for example, in GDP, those activities that have prices, all sorts of weird things happen. Feminist economists and environmental economists have actually written about this quite a bit. Let me give you some examples. If you marry your babysitter, GDP will go down, so do not do it. Do not be tempted to do this, OK? Because an activity that perhaps was before being paid for is still being done but is no longer paid. If you pollute, GDP goes up. Still don\\'t do it, but if you do it, you\\'ll help the economy. Why? Because we have to actually pay someone to clean it. Now, what\\'s also interesting is what happened to finance in the financial sector in GDP. This also, by the way, is something I\\'m always surprised that many economists don\\'t know. Up until 1970, most of the financial sector was not even included in GDP. It was indirectly, perhaps not knowingly, still being seen through the eyes of the Physiocrats as kind of moving stuff around, not actually producing anything new. So only those activities that had an explicit price were included. For example, if you went to get a mortgage, you were charged a fee. That went into GDP and the national income and product accounting. But, for example, net interest payments didn\\'t, the difference between what banks were earning in interest if they gave you a loan and what they were paying out for a deposit. That wasn\\'t being included. And so the people doing the accounting started to look at some data, which started to show that the size of finance and these net interest payments were actually growing substantially. And they called this the \"banking problem.\" These were some people working inside, actually, the United Nations in a group called the Systems of National SNA. They called it the \"banking problem,\" like, \"Oh my God, this thing is huge, and we\\'re not even including it.\" So instead of stopping and actually making that Tableau Economique or asking some of these fundamental questions that also the classicals were asking about what is actually happening, the division of labor between different types of activities in the economy, they simply gave these net interest payments a name. So the commercial banks, they called this \"financial intermediation.\" That went into the NIPA accounts. And the investment banks were called the \"risk-taking activities,\" and that went in. In case I haven\\'t explained this properly, that red line is showing how much quicker financial intermediation as a whole was growing compared to the rest of the economy, the blue line, industry. And so this was quite extraordinary, because what actually happened, and what we know today, and there\\'s different people writing about this, this data here is from the Bank of England, is that lots of what finance was actually doing from the 1970s and \\'80s on was financing itself: finance financing finance. And what I mean by that is finance, insurance and real estate. In fact, in the UK, something like between 10 and 20 percent of finance finds its way into the real economy, into industry, say, into the energy sector, into pharmaceuticals, into the IT sector, but most of it goes back into that acronym, FIRE: finance, insurance and real estate. It\\'s very conveniently called FIRE. Now, this is interesting because, in fact, it\\'s not to say that finance is good or bad, but the degree to which, by having to give it a name, because it actually had an income that was being generated, as opposed to pausing and asking, \"What is it actually doing?\" -- that was a missed opportunity. Similarly, in the real economy, in industry itself, what was happening? And this real focus on prices and also share prices has created a huge problem of reinvestment, again, this real attention that both the Physiocrats and the classicals had to the degree to which the value that was being generated in the economy was in fact being reinvested back in. And so what we have today is an ultrafinancialized industrial sector where, increasingly, a share of the profits and the net income are not actually going back into production, into human capital training, into research and development but being siphoned out in terms of buying back your own shares, which boosts stock options, which is, in fact, the way that many executives are getting paid. And, you know, some share buybacks is absolutely fine, but this system is completely out of whack. These numbers that I\\'m showing you here show that in the last 10 years, 466 of the S and P 500 companies have spent over four trillion on buying back their shares. And what then if you aggregate this up at the macroeconomic level, so if we look at aggregate business investment, which is a percentage of GDP, you also see this falling level of business investment. And this is a problem. This, by the way, is a huge problem for skills and job creation. You might have heard there\\'s lots of attention these days to, \"Are the robots taking our jobs?\" Well, mechanization has for centuries, actually, taken jobs, but as long as profits were being reinvested back into production, then it didn\\'t matter: new jobs appeared. But this lack of reinvestment is, in fact, very dangerous. Similarly, in the pharmaceutical industry, for example, how prices are set, it\\'s quite interesting how it doesn\\'t look at these objective conditions of the collective way in which value is created in the economy. So in the sector where you have lots of different actors -- public, private, of course, but also third-sector organizations -- creating value, the way we actually measure value in this sector is through the price system itself. Prices reveal value. So when, recently, the price of an antibiotic went up by 400 percent overnight, and the CEO was asked, \"How can you do this? People actually need that antibiotic. That\\'s unfair.\" He said, \"Well, we have a moral imperative to allow prices to go what the market will bear,\" completely dismissing the fact that in the US, for example, the National Institutes of Health spent over 30 billion a year on the medical research that actually leads to these drugs. So, again, a lack of attention to those objective conditions and allowing the price system itself to reveal the value. Now, this is not an academic exercise, as interesting as it may be. All this matters how we measure output, to how we steer the economy, to whether you feel that you\\'re productive, to which sectors we end up helping, supporting and also making people feel proud to be part of. In fact, going back to that quote, it\\'s not surprising that Blankfein could say that. He was right. In the way that we actually measure production, productivity and value in the economy, of course Goldman Sachs workers are the most productive. They are in fact earning the most. The price of their labor is revealing their value. But this becomes tautological, of course. And so there\\'s a real need to rethink. We need to rethink how we\\'re measuring output, and in fact there\\'s some amazing experiments worldwide. In New Zealand, for example, they now have a gross national happiness indicator. In Bhutan, also, they\\'re thinking about happiness and well-being indicators. But the problem is that we can\\'t be adding things in. We do have to pause, and I think this should be a moment for pause, given that we see so little has actually changed since the financial crisis, to make sure that we are not also confusing value extraction with value creation, so looking at what\\'s included, not adding more, to make sure that we\\'re not, for example, confusing rents with profits. Rents for the classicals was about unearned income. Today, rents, when they\\'re talked about in economics, is an imperfection towards a competitive price that could be competed away if you take away some asymmetries. Second, we of course can steer activities into what the classicals called the \"production boundary.\" This should not be an us-versus-them, big, bad finance versus good, other sectors. We could reform finance. There was a real lost opportunity in some ways after the crisis. We could have had the financial transaction tax, which would have rewarded long-termism over short-termism, but we didn\\'t decide to do that globally. We can. We can change our minds. We can also set up new types of institutions. There\\'s different types of, for example, public financial institutions worldwide that are actually providing that patient, long-term, committed finance that helps small firms grow, that help infrastructure and innovation happen. But this shouldn\\'t be about output. This shouldn\\'t be about the rate of output. We should also as a society pause and ask: What value are we even creating? And I want to end with the fact that this week we are celebrating the 50th anniversary of the Moon landing. This required the public sector, the private sector, to invest and innovate in all sorts of ways, not around aeronautics. It included investment in areas like nutrition and materials. There were lots of actual mistakes that were done along the way. In fact, what government did was it used its full power of procurement, for example, to fuel those bottom-up solutions, of which some failed. But are failures part of value creation? Or are they mistakes? Or how do we actually also nurture the experimentation, the trial and error and error and error? Bell Labs, which was the R and D laboratory of AT and T, actually came from an era where government was quite courageous. It actually asked AT and T that in order to maintain its monopoly status, it had to reinvest its profits back into the real economy, innovation and innovation beyond telecoms. That was the history, the early history of Bell Labs. So how we can get these new conditions around reinvestment to collectively invest in new types of value directed at some of the biggest challenges of our time, like climate change? This is a key question. But we should also ask ourselves, had there been a net present value calculation or a cost-benefit analysis done about whether or not to even try to go to the Moon and back again in a generation, we probably wouldn\\'t have started. So thank God, because I\\'m an economist, and I can tell you, value is not price. Thank you. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rpunct를 사용해 문장 구별 및 온점 처리\n",
        "rpunct = RestorePuncts()\n",
        "ted_cleaned_text = rpunct.punctuate(ted_fw_cleaned_test)\n",
        "\n",
        "# 전처리 후 문장 길이 및 앞부분 확인\n",
        "print(len(ted_cleaned_text))\n",
        "print(ted_cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JhQt3xvDODm",
        "outputId": "cd2d4b58-3f8b-4397-aaa9-04db9b42ae31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19519\n",
            "Value Creation. Wealth Creation. These are powerful words. Maybe You think of finance, you think of innovation, you think of creativity. But Who are the value creators? If We use that word, we must be implying that some people aren't creating value. Who are they? The Couch potatoes? The Value extractors? The Value destroyers? To Answer this question, we actually have to have a proper theory of value. And I'm here as an economist to break it to you that we've lost our way on this question. Now, don't look so surprised. What I Mean by that is,, we've stopped contesting it. We've stopped actually asking tough questions about what is the difference between value creation and value extraction, productive and unproductive activities. Now, let me give you some context here. 2009 was about a year and a half after one of the biggest financial crises of our time, second only to the 1929 Great Depression, and the CEO of Goldman Sachs said Goldman Sachs Workers are the most productive in the world. Productivity And productiveness, for an economist, actually has a lot to do with value. You're producing stuff, you're producing it dynamically and efficiently. You're also producing things that the world needs, wants and buys. Now, How this could have been said one year after the crisis, which actually had this bank as well as many other banks -- I'm kind of picking on Goldman Sachs here -- at the center of the crisis, because they had actually produced some pretty problematic financial products mainly but not only related to mortgages, which saw many thousands of people actually lose their homes. In 2010, in one month, September, 120,000 people lost their homes through the foreclosures of that crisis. Between 2007 and 2010, 8.8 million people lost their jobs. The Bank also had to then be bailed out by the US taxpayer for the sum of 10 billion dollars. We Didn't hear the taxpayers bragging that they were value creators,, but obviously, having bailed out one of the biggest value-creating productive companies, perhaps they should have. What I Want to do next is ask ourselves how we lost our way, how it could be, actually, that a statement could almost go unnoticed, because it wasn't an after-dinner joke; it was said very seriously. So What I want to do is bring you back 300 years in economic thinking, when, actually, the term was contested. It doesn't mean that they were right or wrong,, but you couldn't call yourself a value creator, a wealth creator. There was a lot of debate within the economics profession.. And What I want to argue is, we've lost our way, and that has actually allowed this term, \"wealth creation\" and \"value,\" to become quite weak and lazy and also easily captured. OK? So Let's start -- I Hate to break it to you -- 300 years ago. Now, What was interesting 300 years ago is the society was still an agricultural type of society. So It's not surprising that the economists of the time, who were called the Physiocrats, actually put the center of their attention to farm labor.. When They said, \"Where does value come from?\" They looked at farming. And They produced what I think was probably the world's first spreadsheet, called the \"Tableau Economique,\" and this was done by François Quesnay, one of the leaders of this movement. And It was very interesting, because they didn't say, \"Farming is the source of value.\" They Then worried about what was happening to that value when it was produced. What The Tableau Economique does -- and I've tried to make it a bit simpler here for you -- is it broke down the classes in society into three. The Farmers, creating value, were called the \"productive class.\" Then Others who were moving some of this value around, but it was useful, it was necessary, These were the merchants; they were called the \"proprietors.\" And Then there was another class that was simply charging the farmers a fee for an existing asset, the land, and they called them the \"sterile class.\" Now, this is a heavy-hitting word if you think what it means: that if too much of the resources are going to the landlords,, you're actually putting the reproduction potential of the system at risk.. And So all these little arrows there were their way of simulating -- again,, spreadsheets and simulators,. these guys were using big data -- They were simulating what would actually happen under different scenarios if the wealth actually wasn't reinvested back into production to make that land more productive and was actually being siphoned out in different ways, or even if the proprietors were getting too much. And What later happened in the 1800s, and this was no longer the Agricultural Revolution but the Industrial Revolution, is that the Classical Economists, and these were Adam Smith, David Ricardo, Karl Marx, the revolutionary, also asked the question \"What is value?\" But It's not surprising that because they were actually living through an industrial era with the rise of machines and factories, they said it was industrial labor. So They had a labor theory of value. But Again,, their focus was reproduction, this real worry of what was happening to the value that was created if it was getting siphoned out. And In \"The Wealth of Nations,\" Adam Smith Had this great example of the pin Factory, where he said if you only have one person making every bit of the pin, at most you can make one pin a day. But If you actually invest in factory production and the division of labor, new thinking -- today, we would use the word \"organizational innovation\" -- then you could increase the productivity and the growth and the wealth of nations.. So He showed that 10 specialized workers who had been invested in, in their human capital, could produce 4,800 pins a day, as opposed to one by an unspecialized worker. And He and his fellow classical economists also broke down activities into productive and unproductive ones. And The unproductive ones weren't -- I Think you're laughing because most of you are on that list, aren't you? Lawyers! I Think he was right about the lawyers. Definitely Not the professors, the letters of all kind people. So Lawyers, professors, shopkeepers, musicians. He Obviously hated the opera. He Must have seen the worst performance of his life the night before writing this book. There's at least three professions up there that have to do with the opera. But This wasn't an exercise of saying, \"Don't do these things.\" It was just, \"What's going to happen if we actually end up allowing some parts of the economy to get too large without thinking about how to increase the productivity of the source of the value that they thought was key, which was industrial labor. And Again,, don't ask yourself, is this right or is this wrong, it was very contested. By Making these lists, it actually forced them also to ask interesting questions. And Their focus, as the focus of the Physiocrats, was, in fact, on these objective conditions of production. They Also looked, for example,, at the class struggle. Their Understanding of wages had to do with the objective,, if you want,, power relationships, the bargaining power of capital and labor. But Again,, factories, machines, division of labor, agricultural land and what was happening to it. So The big revolution that then happened -- and this, by the way,, is not often taught in economics classes -- the big revolution that happened with the current system of economic thinking that we have, which is called \"neoclassical Economics,\" was that the logic completely changed. It changed in two ways. It changed from this focus on objective conditions to subjective ones. Let me explain what I mean by that. Objective, in the way I said. Subjective, in the sense that all the attention went to how individuals of different sorts make their decisions.. OK, so workers are maximizing their choices of leisure versus work. Consumers are maximizing their so-called utility, which is a proxy for happiness,, and firms are maximizing their profits. And The idea behind this was that then we can aggregate this up, and we see what that turns into,, which are these nice, fancy supply-and-demand curves which produce a price, an equilibrium price. It's an equilibrium price, because we also added to it a lot of Newtonian physics equations where centers of gravity are very much part of the organizing principle. But The second point here is that that equilibrium price, or prices, reveal value. So The revolution here is a change from objective to subjective,, but also the logic is no longer one of what is value, how is it being determined,, what is the reproductive potential of the economy, which then leads to a theory of price, but rather the reverse: a theory of price and exchange which reveals value. Now, this is a huge change. And It's not an academic exercise, as fascinating as that might be., It affects how we measure growth. It affects how we steer economies to produce more of some activities, less of others, How we also remunerate some activities more than others. And It also kind of makes you think,, you know,, are you happy to get out of bed if you're a value creator or not, and how is the price system itself if you aren't determining that? I Mentioned it affects how we think about output. If We only include, for example,, in GDP, those activities that have prices, all sorts of weird things happen. Feminist Economists and environmental economists have actually written about this quite a bit. Let Me give you some examples.: If You marry your babysitter, GDP will go down, so do not do it. Do Not be tempted to do this, OK? Because An activity that perhaps was before being paid for is still being done, but is no longer paid. If You pollute, GDP goes up. Still Don't do it, but if you do it, you'll help the economy. Why? Because We have to actually pay someone to clean it. Now, What's also interesting is what happened to finance in the financial sector in GDP. This Also,, by the way,, is something I'm always surprised that many economists don't know. Up Until 1970,, most of the financial sector was not even included in GDP. It was indirectly,, perhaps not knowingly,, still being seen through the eyes of the Physiocrats as kind of moving stuff around, not actually producing anything new. So Only those activities that had an explicit price were included. For Example,, if you went to get a mortgage, you were charged a fee. That went into GDP and the National Income and Product Accounting. But, For example,, net interest payments didn't, the difference between what banks were earning in interest if they gave you a loan and what they were paying out for a deposit. That wasn't being included. And So the people doing the accounting started to look at some data, which started to show that the size of finance and these net interest payments were actually growing substantially. And They called this the \"banking problem.\" These Were some people working inside, actually, the United Nations in a group called the Systems of National SNA. They called it the \"banking problem,\" like, \"Oh my God, this thing is Huge, and we're not even including it.\" So Instead of stopping and actually making that Tableau Economique or asking some of these fundamental questions that also the classicals were asking about what is actually happening, the division of labor between different types of activities in the economy,, they simply gave these net interest payments a name. So The commercial banks, they called this \"financial Intermediation.\" That went into the NIPA accounts. And The investment banks were called the \"risk-taking activities,\" and that went in. In Case I Haven't explained this properly, that red line is showing how much quicker financial intermediation as a whole was growing compared to the rest of the economy,. the Blue line, industry. And So this was quite extraordinary, because what actually happened, and what we know today, and there's different people writing about this, this data here is from the Bank of England, is that lots of what finance was actually doing from the 1970s and '80s on was financing itself: Finance Financing Finance. And What I mean by that is finance, insurance and real Estate. In Fact,, in the UK, something like between 10 and 20 percent of finance finds its way into the real economy, into industry,, say,, into the energy sector,, into pharmaceuticals,, into the IT sector, but most of it goes back into that acronym, FIRE: Finance, Insurance and Real Estate. It's very conveniently called FIRE. Now, This is interesting because, in fact, it's not to say that finance is good or bad,, but the degree to which, by having to give it a name, because it actually had an income that was being generated, as opposed to pausing and asking, \"What is it actually doing?\" -- that was a missed opportunity. Similarly, in the real economy, in industry itself,, what was happening? And This real focus on prices and also share prices has created a huge problem of reinvestment,. Again,, this real attention that both the Physiocrats and the Classicals had to the degree to which the value that was being generated in the economy was in fact being reinvested back in.. And So what we have today is an ultrafinancialized industrial sector, where, increasingly, a share of the profits and the net income are not actually going back into production, into human capital training,, into research and development, but being siphoned out in terms of buying back your own shares, which boosts stock options, which is,, in fact,, the way that many executives are getting paid. And, You know,, some share buybacks is absolutely fine,, but this system is completely out of whack.. These Numbers that I'm showing you here show that in the last 10 years, 466 of the S and P 500 companies have spent over four trillion on buying back their shares. And What? Then If you aggregate this up at the macroeconomic level,, so if we look at aggregate business investment, which is a percentage of GDP, you also see this falling level of business investment. And This is a problem. This, by the way,, is a huge problem for skills and job creation. You Might have heard there's lots of attention these days to, \"Are the robots taking our jobs?\" Well, mechanization has for centuries, actually, taken jobs, but as long as profits were being reinvested back into production, then it didn't matter: new jobs appeared. But This lack of reinvestment is, in fact,, very dangerous. Similarly, In the pharmaceutical industry, for example,, how prices are set,, it's quite interesting how it doesn't look at these objective conditions of the collective way in which value is created in the economy. So In the sector where you have lots of different actors -- public, private, of course,, but also third-sector organizations -- creating value,. the way we actually measure value in this sector is through the price system itself. Prices Reveal value. So When, recently, the price of an antibiotic went up by 400 percent overnight, and the CEO was asked, \"How can you do this? People actually need that antibiotic. That's unfair.\" He Said, \"Well, we have a moral imperative to allow prices to go what the market will bear,\" completely dismissing the fact that in the US, for example,, the National Institutes of Health spent over 30 billion a year on the medical research that actually leads to these drugs.. So, again,, a lack of attention to those objective conditions and allowing the price system itself to reveal the value.. Now, this is not an academic exercise, as interesting as it may be.. All This matters how we measure output, to how we steer the economy, to whether you feel that you're productive, to which sectors we end up helping, supporting, and also making people feel proud to be part of.. In Fact,, going back to that quote, it's not surprising that Blankfein could say that. He was right. In The way that we actually measure production, productivity and value in the economy, of course: Goldman Sachs Workers are the most productive. They are in fact earning the most. The Price of their labor is revealing their value. But This becomes tautological, of course. And So there's a real need to rethink. We need to rethink how we're measuring output, And in fact, there's some amazing experiments worldwide. In New Zealand, For example,, they now have a gross National Happiness indicator. In Bhutan, Also,, they're thinking about happiness and well-being indicators. But The problem is that we can't be adding things in. We do have to pause, and I think this should be a moment for pause, given that we see so little has actually changed since the financial crisis, to make sure that we are not also confusing value extraction with value creation, So looking at what's included, not adding more, to make sure that we're not, for example,, confusing rents with profits. Rents for the Classicals was about unearned income. Today, rents, when they're talked about in economics, is an imperfection towards a competitive price that could be competed away if you take away some asymmetries. Second, we, of course, can steer activities into what the Classicals called the \"production boundary.\" This should not be an Us-versus-them, big, bad finance versus good, other sectors. We Could reform finance. There was a real lost opportunity in some ways after the crisis. We could have had the financial transaction tax, which would have rewarded long-termism over short-termism, but we didn't decide to do that globally. We can. We can change our minds. We can also set up new types of institutions. There's different types of, for example,, public financial institutions worldwide that are actually providing that patient, long-term, committed finance that helps small firms grow, that help infrastructure and innovation happen. But This shouldn't be about output. This shouldn't be about the rate of output. We should also as a society pause and ask: What Value are we even creating? And I Want to end with the fact that this week we are celebrating the 50th anniversary of the Moon landing. This required the public sector, the private sector, to invest and innovate in all sorts of ways, not around aeronautics. It included investment in areas like nutrition and materials. There were lots of actual mistakes that were done along the way. In Fact,, what government did was it used its full power of procurement, for example,, to fuel those bottom-up solutions, of which some failed. But Are failures part of value creation? Or Are they mistakes? Or How do we actually also nurture the experimentation, the trial and error and error and error? Bell Labs, which was the R and D laboratory of AT and T, actually came from an era where government was quite courageous. It actually asked AT and T that in order to maintain its monopoly status, it had to reinvest its profits back into the real economy, innovation and innovation beyond telecoms. That was the history, the early history of Bell Labs. So How we can get these new conditions around reinvestment to collectively invest in new types of value directed at some of the biggest challenges of our time, like climate change? This is a key question. But We should also ask ourselves, had there been a net present value calculation or a cost-benefit analysis done about whether or not to even try to go to the Moon and back again in a generation, we probably wouldn't have started. So Thank God, because I'm an economist, and I can tell you, value is not price. Thank You.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Golden Revuew\n",
        "\n",
        "*   전처리 이전 : gr_subtitle_merge : 6,735\n",
        "*   전처리 이후 : gr_cleaned_text : 6,756"
      ],
      "metadata": {
        "id": "KB5qnr6GDUaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일단 자막 space로 split하고, '['나 '('가 포함되어 있는건 삭제, 아니면(보통 문자열이면) cleaned 된 문자열 리스트에 추가 \n",
        "\n",
        "splited = gr_subtitle_merge.split()\n",
        "gr_bracket_cleaned_subtitle = \"\"\n",
        "for i in splited :\n",
        "  if '[' in i or '(' in i :\n",
        "    pass\n",
        "  else :\n",
        "    gr_bracket_cleaned_subtitle += (i + ' ')\n",
        "\n",
        "len(gr_bracket_cleaned_subtitle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22u_udBbDafa",
        "outputId": "d42fc714-060f-4698-ddb0-6f9fd80cc814"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6735"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fw_cleaned_test에 클린이 된 text가 할당됨\n",
        "gr_fw_cleaned_test = re.sub(fw_list, \" \", gr_bracket_cleaned_subtitle)\n",
        "gr_fw_cleaned_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "blatuw4dDbwh",
        "outputId": "1c4c4785-3c5c-4caa-fc8f-81e41fc735a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hi guys welcome back to golden reviewer today i'm going to show you how to disable game optimization service on samsung smartphone first you search for this netgrad app in play store and install it then you open the app go to settings advanced options and select manage system apps because the apps we are going to block are all system apps then we search for the keyword game there are three apps we want to disable one is game optimization service another is game booster and game launcher as well for game booster plus and game plugins i think they they don't matter but to be safe here i uninstall them because they are not system apps so you can uninstall them but for the three apps we are going to block their system apps so click these two buttons here to block them from accessing the internet and then you press the button on the top left corner to make sure that netgard is enabled then you accept all the promotes and until this key in your status bar that means it's running the next step we are going to do is we go to the settings of these apps you can actually directly go there from within the netguard which is very convenient go to storage and clear data for all three apps the rationale is that the game optimization service actually download a full list of app signatures from samsung server when it's running in the background and it will throttle according to this list of app names so we block them from accessing the internet so they cannot download the app name list and then we clear the app data to remove any already downloaded app name list so that the game optimization service does not know which apps to throttle and finally turn off your wi-fi or data and ending and restart your phone this is because we want to make sure we run the netgard app before the game optimization service can auto start from background when the device first boots up right so turn off your internet so even the game optimization service put up it won't be able to download anything and the first thing we're going to do is to turn on the netgraph to make sure that they are blocked and then we can turn on the wi-fi here you can see i switch on the toggle again to make sure and then you will be able to see this notification in your notification panel right that means the netguard is already working to block the internet access and now you can turn on your wi-fi and you can continue to play your games without game optimization service and next i'll show you how much a difference it makes to actually turn on and off the game optimization service okay so here is my galaxy s22 ultra it's the snapdragon version of course i'll test the exynos version later on in another video maybe but here this is to show you how much a difference the game optimization service actually makes right so the one on the left is which game optimization service on and one on the right is which is off okay so first thing you notice is how much smoother the one on the right is and this is confirmed by the fps reading as well we are seeing something around 50 now at the beginning we even have 60 on the other hand on the left we can see if we leave game optimization series on the fps is only around 30 to 40 maybe at most it can reach 40 something but most of the time is from 30 to 40 right this also was confirmed by our previous test where if you play for 10 minutes the average fps is only around 32 so there's a clear difference in terms of frame rate but we also see another difference in power consumption if you look at the power consumption on the right it's much higher than the one on the left okay so higher performance requires higher power consumption and this will result in more heat as well okay so turning off game optimization service does not magically make your device just better in every way right yes it can run the game smoother but it will generate more heat and it will use more batching and in the end if your device becomes too hot thermal throttling will still keep it and it will still restrict your performance and your fps will become lower as well actually we are seeing that chart here right you will notice that the one on the right has started to heat up and the throttle the fps starts to drop it's still higher than the one on the left with gos on but that's physics you cannot change it your device heats up you have to throttle otherwise it will eventually melt or explode right but then this gives us more possibility if you use a cooler or if you are staying at a cooler place and i think in that case your game will run much smoother and that was not possible with gos on and here we can use this cpu float to understand this issue more clearly if you look at the cpu gpu frequency you can see that with gos on the cpu is restricted to a very low frequency no matter what you do the frequency won't go up and it's the same story for gpu with gos on the gpu frequency actually makes it out at around 300 megahertz while with gos off it can actually reach around 500 megahertz right and then the power consumption is very different as well with gos on it's own only about 1200 ma but with gos off we see that the current can go up to 2000 milliampere right so lastly we can see the performance comparison the fps record with gos on and off we can see that's a very clear difference with gos off we get more than 10 fps more and the game indeed run much smoother and then for power we see that the power consumption is also much higher right we see an overall more than 50 percent power consumption increase and especially for the first two minutes when the device has not started to throttle the average power may be even more than eight watts which is pretty high and we see this gradual throttling pattern as we see from other devices so but if we have gos on the power is restricted to a very low level even from the beginning so we don't see any change in power consumption there is no throttling at all because it started with a low performance alright guys so that's all for today's video and i have to say that i took the inspiration from a korean channel and i'll leave the link down below if you understand korean you can go check out so i think he actually invented this blocking the internet access method so i uh took it and i use maybe a better free software to do this and to be honest this method for now is still pretty tedious and it's not that practical but of course i'll try to do more tests on this and we are all waiting for the system update from samsung who actually promised to fix the issue all right so that's all for today thanks for watching and see you next time \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rpunct를 사용해 문장 구별 및 온점 처리\n",
        "rpunct = RestorePuncts()\n",
        "gr_cleaned_text = rpunct.punctuate(gr_fw_cleaned_test)\n",
        "\n",
        "# 전처리 후 문장 길이 및 앞부분 확인\n",
        "print(len(gr_cleaned_text))\n",
        "print(gr_cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95gmhseiDcr6",
        "outputId": "4ceb38aa-4343-4d89-d360-8c5573fa0bf4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "6756\n",
            "Hi guys! Welcome back to Golden Reviewer today. I'm going to show you how to disable game Optimization service on Samsung smartphone. First you search for this Netgrad app in play store and install it. Then you open the app, go to settings advanced options, and select manage system apps because the apps we are going to block are all system apps. Then we search for the keyword game. There are three apps we want to disable. One is game optimization service, another is game booster and game launcher as well. For game booster plus and game plugins, I think they they don't matter, but to be safe here, I uninstall them because they are not system apps so you can uninstall them. But for the three apps, we are going to block their system apps. So click these two buttons here to block them from accessing the internet. and then you press the button on the top left corner to make sure that Netgard is enabled. Then you accept all the promotes and until this key in your status bar, that means it's running. The next step we are going to do is we go to the settings of these apps. You can actually directly go there from within the Netguard, which is very convenient. Go to storage and clear data for all three apps. The rationale is that the game Optimization service actually download a full list of app signatures from Samsung server when it's running in the background and it will throttle according to this list of app names. So we block them from accessing the internet so they cannot download the app name list and then we clear the app data to remove any already downloaded app name list so that the Game Optimization service does not know which apps to throttle and finally turn off your Wi-fi or data and ending and restart your phone. This is because we want to make sure we run the Netgard app before the game optimization service can auto start from background when the device first boots up right. So turn off your Internet so even the game optimization service put up, it won't be able to download anything and the first thing we're going to do is to turn on the Netgraph to make sure that they are blocked and then we can turn on the Wi-fi Here you can see I switch on the toggle again to make sure and then you will be able to see this notification in your notification panel right? That means the Netguard is already working to block the internet access and now you can turn on your Wi-fi and you can continue to play your games without game Optimization service. And next I'll show you how much a difference it makes to actually turn on and off the game Optimization service. Okay, so here is my Galaxy S22 Ultra. It's the Snapdragon version. Of course I'll test the Exynos version later on in another video maybe, but here this is to show you how much a difference the game optimization service actually makes, right? So the one on the left is which game optimization service on and one on the right is which is off. Okay, so first thing you notice is how much smoother the one on the right is and this is confirmed by the Fps reading as well. we are seeing something around 50 now at the beginning we even have 60. On the other hand, on the left, we can see if we leave. game optimization series on the Fps is only around 30 to 40. Maybe at most it can reach 40 something, but most of the time is from 30 to 40 right? This also was confirmed by our previous test where if you play for 10 minutes, the average Fps is only around 32. So there's a clear difference in terms of frame rate. But we also see another difference in power consumption. If you look at the power consumption on the right, it's much higher than the one on the left. Okay, so higher performance requires higher power consumption and this will result in more heat as well. Okay, so turning off game optimization service does not magically make your device just better in every way, right? Yes, it can run the game smoother, but it will generate more heat and it will use more batching. And in the end, if your device becomes too hot, thermal throttling will still keep it and it will still restrict your performance and your Fps will become lower as well. Actually, we are seeing that chart here right? You will notice that the one on the right has started to heat up and the throttle the Fps starts to drop. It's still higher than the one on the left with Gos On, but that's physics. You cannot change it. Your device heats up, you have to throttle, otherwise it will eventually melt or explode, right? But then this gives us more possibility if you use a cooler or if you are staying at a cooler place and I think in that case your game will run much smoother and that was not possible with Gos on. And here we can use this Cpu float to understand this issue more clearly. If you look at the Cpu Gpu frequency, you can see that with Gos on, the Cpu is restricted to a very low frequency. No matter what you do, the frequency won't go up and it's the same story for Gpu. With Gos on, the Gpu frequency actually makes it out at around 300 megahertz, while with Gos off, it can actually reach around 500 megahertz right? And then the power consumption is very different as well. With Gos on it's own, only about 1200 ma, But with Gos off, we see that the current can go up to 2000 milliampere, right? So lastly, we can see the performance comparison the Fps record with Gos on and off we can see that's a very clear difference. With Gos Off, we get more than 10 Fps more and the game indeed run much smoother. And then for power, we see that the power consumption is also much higher, right? We see an overall more than 50 percent power consumption increase, and especially for the first two minutes when the device has not started to throttle. The average power may be even more than eight watts, which is pretty high, and we see this gradual throttling pattern as we see from other devices. So, but if we have Gos on, the power is restricted to a very low level, even from the beginning, so we don't see any change in power consumption. There is no throttling at all because it started with a low performance. Alright guys, so that's all for today's video and I have to say that I took the inspiration from a Korean channel and I'll leave the link down below. If you understand Korean, you can go check out. So I think he actually invented this blocking the internet access method so I uh, took it and I use maybe a better free software to do this. and to be honest, this method for now is still pretty tedious and it's not that practical. But of course I'll try to do more tests on this and we are all waiting for the system update from Samsung who actually promised to fix the issue. All right, So that's all for today. Thanks for watching and see you next time!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) WION\n",
        "\n",
        "*   전처리 이전 : wion_subtitle_merge : 2,178\n",
        "*   전처리 이후 : wion_cleaned_text : 2,207"
      ],
      "metadata": {
        "id": "SgWa0pgxDlK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일단 자막 space로 split하고, '['나 '('가 포함되어 있는건 삭제, 아니면(보통 문자열이면) cleaned 된 문자열 리스트에 추가 \n",
        "\n",
        "splited = wion_subtitle_merge.split()\n",
        "wion_bracket_cleaned_subtitle = \"\"\n",
        "for i in splited :\n",
        "  if '[' in i or '(' in i :\n",
        "    pass\n",
        "  else :\n",
        "    wion_bracket_cleaned_subtitle += (i + ' ')\n",
        "\n",
        "len(wion_bracket_cleaned_subtitle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF3coBm6DqYz",
        "outputId": "eabb26e7-4150-4591-edab-26d82531e14b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2178"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fw_cleaned_test에 클린이 된 text가 할당됨\n",
        "wion_fw_cleaned_test = re.sub(fw_list, \" \", wion_bracket_cleaned_subtitle)\n",
        "wion_fw_cleaned_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "MOiyGzWtDsjM",
        "outputId": "11885bb7-45a2-4f06-eb96-bcef5c2bf358"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the russia-ukraine war could be headed for a frozen conflict the situation on ground is evolving in such a manner that a frozen conflict is more likely the way forward to end the war experts say that humanitarian sees fires hinting towards that possibility as well but first let's tell you what a frozen conflict is a frozen conflict is a situation in which active armed conflict has ended but without a formal peace agreement or other political framework in place frozen conflict occurs in regions of a country no longer controlled by its central authorities these zones then remain under jurisdiction of the separators as a result states backing the separators run their puppet governments moreover the lack of non-violent solutions failed to permanently end conflict this form of conflict was unique to a few former soviet republics especially during the collapse of the soviet union in 1991. now russia is often accused of destabilizing its former soviet neighbors to keep them in its sphere of influence and stop them from integrating with the nato and eu that is why russia annex crimea destabilized eastern ukraine in 2014 and days before russia invaded ukraine it recognized the two separatist regions donesk and luhansk on the 21st of february moldova's breakaway territory and georgia's rebel regions of south ossetia and abkhazia are also examples of this policy by russia more recent is the example of nagorno-karabakh a disputed region between armenians and the azeri's in 2020 a ceasefire agreed that azerbaijan would regain the territories it lost earlier and gained during its military maneuvers in 2020 but in georgia moldova azerbaijan now donbass in ukraine moscow is known to have favored separatist sentiments in all of these regions this is because the kremlin aims to create a conflict without a solution only so that it can step in to keep peace on its own terms while many may believe that the war in ukraine has begun and there is still some time for a frozen conflict the recent humanitarian ceasefires in ukraine indicate a strong possibility of it hello beyond is now available in your country download the app now get all the news on the move \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rpunct를 사용해 문장 구별 및 온점 처리\n",
        "rpunct = RestorePuncts()\n",
        "wion_cleaned_text = rpunct.punctuate(wion_fw_cleaned_test)\n",
        "\n",
        "# 전처리 후 문장 길이 및 앞부분 확인\n",
        "print(len(wion_cleaned_text))\n",
        "print(wion_cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk2uNqHIDtcT",
        "outputId": "c03f2962-167d-4a11-cef4-76b49bde47b8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2207\n",
            "The Russia-ukraine war could be headed for a frozen conflict. The situation on ground is evolving in such a manner that a frozen conflict is more likely the way forward to end the war. Experts say that humanitarian sees fires hinting towards that possibility as well. But first, let's tell you what a frozen conflict is: A frozen conflict is a situation in which active armed conflict has ended, but without a formal peace agreement or other political framework in place. Frozen conflict occurs in regions of a country no longer controlled by its central authorities. These zones then remain under jurisdiction of the separators. As a result, states backing the separators run their puppet governments. Moreover, the lack of non-violent solutions failed to permanently end conflict. This form of conflict was unique to a few former Soviet republics, especially during the collapse of the Soviet Union in 1991.. now, Russia is often accused of destabilizing its former Soviet neighbors to keep them in its sphere of influence and stop them from integrating with the Nato and Eu. That is why Russia annex Crimea, destabilized Eastern Ukraine in 2014, And days before Russia invaded Ukraine, it recognized the two separatist regions Donesk and Luhansk on the 21st of February. Moldova's breakaway territory and Georgia's rebel regions of South Ossetia and Abkhazia are also examples of this policy by Russia. More recent is the example of Nagorno-karabakh a disputed region between Armenians and the Azeri's In 2020, a ceasefire agreed that Azerbaijan would regain the territories it lost earlier and gained during its military maneuvers in 2020. But in Georgia, Moldova, Azerbaijan now Donbass in Ukraine, Moscow is known to have favored separatist sentiments in all of these regions. This is because the Kremlin aims to create a conflict without a solution, only so that it can step in to keep peace on its own terms. While many may believe that the war in Ukraine has begun and there is still some time for a frozen conflict, the recent humanitarian ceasefires in Ukraine indicate a strong possibility of it. Hello Beyond is now available in your country. Download the app now. get all the news on the move.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 영상 별 전처리 생성 변수\n",
        "1.   yh_cleaned_text - 자막 길이 : 3,458\n",
        "2.   ted_cleaned_text - 자막 길이 : 19,519\n",
        "3. gr_cleaned_text - 자막 길이 : 6,756\n",
        "4. wion_cleaned_text - 자막 길이 : 2,207"
      ],
      "metadata": {
        "id": "L05An-I5EGSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 요약 모델 테스트\n",
        "1.  \"facebook/bart-large-cnn\" 모델 - 생성요약\n",
        "2. \"sshleifer/distilbart-cnn-12-6\" 모델\n",
        "3.\"human-centered-summarization/financial-summarization-pegasus\" 모델\n",
        "4. \"Bert-extractive-summarization\" 모델"
      ],
      "metadata": {
        "id": "ZeY1P099D6FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. \"facebook/bart-large-cnn\" 모델 사용한 요약 생성\n",
        "- 해당 모델의 경우 input으로 4000단어 미만밖에 받지 못함"
      ],
      "metadata": {
        "id": "lSxvrAhWFHSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"facebook/bart-large-cnn\"모델 사용해서 요약 생성하는 함수\n",
        "def summarize(subtitle):\n",
        "  summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summarized_subtitle = summarizer(subtitle, max_length=300, min_length=30, do_sample=False)\n",
        "\n",
        "  return summarized_subtitle[0][\"summary_text\"]\n"
      ],
      "metadata": {
        "id": "ttuRKDYjE8dg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. \"sshleifer/distilbart-cnn-12-6\" 모델 테스트\n",
        "\n",
        "- api로 테스트 가능 https://huggingface.co/sshleifer/distilbart-cnn-12-6\n",
        "\n",
        "- long documentation 요약 \n",
        "  - 튜토리얼 영상 https://www.youtube.com/watch?v=78KjuKYiF6s&t=228s\n",
        "  - https://gist.github.com/saprativa/b5cb639e0c035876e0dd3c46e5a380fd\n",
        "\n",
        "\n",
        "허깅페이스에서 api 이용해서 4000자 이하 자막 테스트 한 결과, 퀄이 좋지는 않음\n",
        "\n",
        "\n",
        "\n",
        "'As we watch the spike that we've been seeing in commodity prices, we have heard a renewal of calls in the United States to drill more that the U.S. should be energy independent . Rick Newman has been looking into that question and it's not as easy as that I think is the is the bottom line .'"
      ],
      "metadata": {
        "id": "pqgFBxg-FSIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model and Tokenizer\n",
        "# import and initialize the tokenizer and model from the checkpoint\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "checkpoint = \"sshleifer/distilbart-cnn-12-6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "Tdj6-ZSGFT5a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. \"human-centered-summarization/financial-summarization-pegasus\" 모델 테스트\n",
        "\n",
        "- api 테스트 : https://huggingface.co/human-centered-summarization/financial-summarization-pegasus\n",
        "\n",
        "- 결과\n",
        "'Oil, coal, solar and wind are all forms of energy in the United States. `"
      ],
      "metadata": {
        "id": "CKXHlbWIGB_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. \"Bert-extractive-summarization\" 모델 테스트"
      ],
      "metadata": {
        "id": "KVPQZSiyGFbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "KUbfTm_TOSfL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (1) Yahoo"
      ],
      "metadata": {
        "id": "tp-4FosHEVnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"facebook/bart-large-cnn\" 모델"
      ],
      "metadata": {
        "id": "_BCps5oHGOPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 없이 요약 걸리는 시간\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "summarize(yh_subtitle_merge)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCqJh86OE_hU",
        "outputId": "cba44a21-d69d-4573-c373-14d12e4c4fab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43.18373131752014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약 길이\n",
        "print(len(summarize(yh_subtitle_merge)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRVjV-WaOi0E",
        "outputId": "a0016beb-35fd-47a8-ba10-7ff35272a6dc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약\n",
        "print(summarize(yh_subtitle_merge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHHgzt--O0I-",
        "outputId": "34c3c289-5fa7-470c-d8a1-930425b1ff4f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rick newman: There's a lot of confusion about this so i'm doing some reporting to try to bust some of these myths so people think the united states used to be energy independent. We are not oil independent we have we still uh consume considerably more oil than we produce and we have not been oil dependent since the early days of uh the oil economy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 후 요약 걸리는 시간\n",
        "start = time.time()\n",
        "\n",
        "summarize(yh_cleaned_text)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXnH0q1ME_8j",
        "outputId": "1c0d9a12-ee10-4120-d180-31f005f01cdb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33.42358994483948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약 길이\n",
        "print(len(summarize(yh_cleaned_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGsJfFyAOslV",
        "outputId": "1380dbda-4e01-4c52-c460-03a992733f46"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약\n",
        "print(summarize(yh_cleaned_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgpL_gyjO89j",
        "outputId": "3a3a50d5-dde9-4661-eec1-8d43bb91a57d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The U.S. has not been oil dependent since since the early days of the oil economy, which goes all the way back to the 1860s. We have imported more oil than we have produced for something like the last 45 or 50 years, and that's probably going to continue. A lot of U.s oil actually gets exported to other countries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"sshleifer/distilbart-cnn-12-6\" 모델"
      ],
      "metadata": {
        "id": "cMurGikrGTge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "yh_FileContent = yh_subtitle_merge\n",
        "yh_FileContent\n",
        "len(yh_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVaYkxoiSXDw",
        "outputId": "f99c9418-2e65-44b8-e2b9-99ecb664e6be"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3491"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert entire document to sentences using 'nltk' \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.tokenize.sent_tokenize(yh_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpLvf3CVSjYR",
        "outputId": "23505c8f-13ec-4eec-893c-4f82e1c469b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-27Ksy0HSn5W",
        "outputId": "69320a47-d338-4ef8-f9da-39b60f48c01c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0Giv71ASp3q",
        "outputId": "ed292a08-dca2-4b70-ab3c-5560b7713cec"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "741"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer(yh_FileContent).input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vT3w_CXSqNc",
        "outputId": "7be312c0-a74a-46bf-abf4-a5a9cfcd0375"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "742"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without special tokens added \n",
        "sum([len(tokenizer.tokenize(c)) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bahuSz_BSrV3",
        "outputId": "a55d7ef2-67c4-48cd-ddd6-cf3131193355"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "739"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(yh_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kymyMKoSs4e",
        "outputId": "adc1bc0d-9d08-457e-bc96-6e615fdeb8da"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "740"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4qWOl1jSuJr",
        "outputId": "50a60d1d-e842-41ff-a09f-df5195cde0bb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " As we watch the spike that we've been seeing in commodity prices we have heard a renewal of calls in the united states to drill more that the u.s should be energy independent. The united states have not been oil dependent since the 1860s so we have imported more oil than we have produced for something like the last 45 or 50 years.\n",
            "18.068843364715576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "yh_cleaned_FileContent = yh_cleaned_text\n",
        "yh_cleaned_FileContent\n",
        "len(yh_cleaned_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ek0N_XTJ52",
        "outputId": "76f44a33-c5f9-4676-ea74-39494fdd2b7c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3458"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(yh_cleaned_FileContent)"
      ],
      "metadata": {
        "id": "g3oh6xh3UctC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcVFOkQkUedW",
        "outputId": "6c5ffb94-9da1-4492-bc47-898010bd112a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqO6Q57sUga-",
        "outputId": "ca652843-4aba-4f68-e4f9-a9f27f9c6129"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "775"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(yh_cleaned_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSJfQC24UiWE",
        "outputId": "c6ccf99e-b41a-4724-836a-450aa1ee16fb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "773"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecKvGbtgUkk4",
        "outputId": "de0b4fc9-1f7c-42b2-8f41-ffb239443fba"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Rick Newman has been looking into that question and it's not as easy as that I think is the is the bottom line, right Rick? There's a lot of confusion about this, so I'm doing some reporting to try to bust some of these myths. We consume more than we produce, which you could say makes us independent, but we also participate in global markets. We still import energy, and in terms of oil, we are not oil independent.\n",
            "19.027642250061035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"human-centered-summarization/financial-summarization-pegasus\" 모델"
      ],
      "metadata": {
        "id": "jBpFAJdXGYLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(yh_subtitle_merge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rm5hFzsXbV7",
        "outputId": "12a5214c-14c3-4eab-cc42-820159037c04"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "as we watch the spike that we've been seeing in commodity prices we have heard a renewal of calls in the united states to drill more that the u.s should be energy independent our rick newman has been looking into that question um and it's it's not sort of as easy as that i think is the is the bottom line right rick there's a lot of confusion about this so i'm doing some reporting to try to bust some of these myths so people think the united states used to be energy independent so here's what that actually means when you combine all forms of energy that we produce in the united states that's oil but also natural gas also coal also renewables such as solar and wind yes we do consume more than we produce which i guess you could say makes us independent but we also participate in global markets which means we export a lot of that we still import energy and in terms of just oil we are not oil independent we have we still uh consume considerably more oil than we produce and we we have not been oil dependent since uh since the early days of uh the oil economy uh which goes all the way back to the 1860s so we have imported more oil than we have um produced for something like the last 45 or 50 years and that's probably going to continue and even though we we do need oil here in the united states a lot of u.s oil actually gets exported to other countries this is a global market um there's no way that the u.s president can put up walls so that all all the oil that's produced in the united states stays in the united states so that means we are susceptible to what happens in global oil markets we cannot fence ourselves off from what's going on in the rest of the world when it comes to oil prices hey rick you are a very much a forward thinker here gas prices record high i paid about over five dollars over the weekend for my premium unleaded that's just what my car takes that's what it recommends uh what do you think those will do or what do you think it means the midterm elections it's a great question and i'm not sure because uh ordinarily when gas prices go this high if they stay that high for any period of time it's just doom for the party that's in control but we saw some polls over the last few days with people americans saying i think more than 60 percent in one poll said it's worth paying more for gas prices as a way of uh tolerating the sanctions that are on russia and doing our part if you will to help out so there is a portion of voters who actually seem to be okay with this for the time being and we've also seen president biden's approval rating which bottomed out around 41 percent has actually ticked up lately so there does seem to be um some support for sort of uh rallying to the cause here i think it's going to be a different story if if gas prices stay this higher go higher for you know for the next eight months leading into the midterm elections because this is real pain for people uh you know who are dependent on cars and sazi i could have told you man um get a car that doesn't require premium fuel i it's serious how about one that just requires less fuel generally maybe get a hybrid or electric right coming around electric i mean i can't be ganged up here it's monday morning it's going to be a long week i think it i think it's time to comment i think it's no comment on wednesday keep it moving we're gonna we're gonna drag you into uh the 21st century whether you like it or not arthric newman thank you so much appreciate it \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(yh_cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DmHKXpRXfOJ",
        "outputId": "2037b8e8-60a2-4cfd-df0e-99f41ebfe7e6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As we watch the spike that we've been seeing in commodity prices, we have heard a renewal of calls in the United States to drill more that the U.s should be energy independent. Our Rick Newman has been looking into that question and it's It's not as easy as that I think is the is the bottom line, right Rick? There's a lot of confusion about this, so I'm doing some reporting to try to bust some of these myths. So people think the United States used to be energy independent. So here's what that actually means when you combine all forms of energy that we produce in the United States, that's oil, but also natural gas. also coal. also renewables such as solar and wind. Yes, we do consume more than we produce, which you could say makes us independent. but we also participate in global markets, which means we export a lot of that. We still import energy, and in terms of oil, we are not oil independent. We have. We still consume considerably more oil than we produce, and we we have not been oil dependent since since the early days of the oil economy, which goes all the way back to the 1860s. So we have imported more oil than we have produced for something like the last 45 or 50 years, and that's probably going to continue. And even though we we do need oil here in the United States, a lot of U.s oil actually gets exported to other countries. This is a global market. There's no way that the U.s President can put up walls so that all all the oil that's produced in the United States stays in the United States. So that means we are susceptible to what happens in global oil markets. We cannot fence ourselves off from what's going on in the rest of the world when it comes to oil prices. Hey Rick, you are a very much a forward thinker here. Gas prices record high. I paid about over five dollars over the weekend for my premium unleaded. That's what my car takes. That's what it recommends. What do you think those will do? Or what do you think it means The midterm elections? It's a great question, and I'm not sure because ordinarily when gas prices go this high, if they stay that high for any period of time, it's doom for the party that's in control. But we saw some polls over the last few days with people Americans saying i think more than 60 percent in one poll said it's worth paying more for gas prices as a way of tolerating the sanctions that are on Russia and doing our part if you will to help out. So there is a portion of voters who actually seem to be okay with this for the time being. and we've also seen President Biden's approval rating which bottomed out around 41 percent has actually ticked up lately, so there does seem to be some support for, uh, rallying to the cause here. I think it's going to be a different story if if gas prices stay this higher, go higher for for the next eight months leading into the midterm elections because this is real pain for people you know who are dependent on cars and Sazi. I could have told you, man, get a car that doesn't require premium fuel. I, it's serious. How about one that requires less fuel Generally, Maybe get a hybrid or electric right? Coming around electric, I can't be ganged up here. It's Monday morning. it's going to be a long week. I think it. I think it's time to comment. I think it's no comment on Wednesday. Keep it moving, we're gonna. We're gonna drag you into the 21st century whether you like it or not. Arthric Newman thank you so much Appreciate it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"Bert-extractive-summarization\" 모델"
      ],
      "metadata": {
        "id": "fzjStwsKGbLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (2) TED"
      ],
      "metadata": {
        "id": "uNOKrf2oa0v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"facebook/bart-large-cnn\" 모델"
      ],
      "metadata": {
        "id": "vaQdQT4Ta6N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 없이 요약 걸리는 시간\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "summarize(ted_subtitle_merge)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "Uf6SBNUna-K4",
        "outputId": "bb7fd7b2-1d46-4a5a-f2a4-72152a69d128"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4415 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-4d4d00b0e12d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mted_subtitle_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-823e812f1d16>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(subtitle)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msummarized_subtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msummarized_subtitle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         if (\n\u001b[1;32m    140\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_shape, past_key_values_length)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return F.embedding(\n\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약 길이\n",
        "print(len(summarize(ted_subtitle_merge)))"
      ],
      "metadata": {
        "id": "NNsgA1V_a-xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약\n",
        "print(summarize(ted_subtitle_merge))"
      ],
      "metadata": {
        "id": "eBcUATxma--3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 후 요약 걸리는 시간\n",
        "start = time.time()\n",
        "\n",
        "summarize(ted_cleaned_text)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "id": "PSz28glJa_Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약 길이\n",
        "print(len(summarize(ted_cleaned_text)))"
      ],
      "metadata": {
        "id": "_OYLWSdFbNxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약\n",
        "print(summarize(ted_cleaned_text))"
      ],
      "metadata": {
        "id": "Xe1ICuP1bOOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"sshleifer/distilbart-cnn-12-6\" 모델"
      ],
      "metadata": {
        "id": "nsRIcMCQcENX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "ted_FileContent = ted_subtitle_merge\n",
        "ted_FileContent\n",
        "len(ted_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khp8bTEhcGc2",
        "outputId": "0313f95c-5de0-4307-9d3b-b89c094a023b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19767"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert entire document to sentences using 'nltk' \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.tokenize.sent_tokenize(ted_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKS95UShcHjF",
        "outputId": "ff6da32e-0ac8-4014-80e5-d2978adeb404"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDT7pwAcIji",
        "outputId": "02b1a1a4-cb1f-4681-d385-a034497d2ac3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_jwmMfLcLCP",
        "outputId": "2e486bd0-80b1-40f4-d4bc-f6cbe8430700"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4422"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(ted_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtWgLBZgcM4j",
        "outputId": "de28bc4c-2239-44f4-8306-49530b175161"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4413 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4413"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXh3VzFOcNxR",
        "outputId": "297304ca-e804-486e-c02e-aeca0df4905d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Goldman Sachs CEO said Goldman Sachs workers are the most productive in the world. Peter Bergen: We've stopped asking tough questions about value creation and value extraction. He says the term \"wealth creation\" and \"value\" have become weak and lazy. Bergen says it's important to know what is the difference between productive and unproductive activities.\n",
            " Adam Smith, David Ricardo, Karl Marx and others asked the question \"What is value?\" They had a labor theory of value, but again, their focus was reproduction. Adam Smith had this really great example of the pin factory where he said if you only have one person making every bit of the. But if you actually invest in factory production and the division of labor, new thinking.\n",
            " Up until 1970, most of the financial sector was not even included in GDP. The U.N. called it the \"banking problem\" because it was seen as just kind of moving stuff around, not actually producing anything new. In the UK, between 10 and 20 percent of finance finds its way into the real economy, into industry, pharmaceuticals, into the IT sector.\n",
            " In the last 10 years,466 of the S and P 500 companies have spent over four trillion dollars on just buying back their shares. This, by the way, is a huge problem for skills and job creation. In the pharmaceutical industry, how prices are set, it's quite interesting how it doesn't look at these objective conditions of the collective way in which value is created in the economy. We need to rethink how we're measuring output, and in fact there's some amazing experiments worldwide.\n",
            " This week we are celebrating the 50th anniversary of the Moon landing. It required the public sector to invest and innovate in all sorts of ways, not just around aeronautics. But this shouldn't just be about output, but what value are we even creating? We should also ask ourselves, had there been a net-benefit analysis done about whether or not to go to the Moon and back again in a generation, we probably wouldn't have started.\n",
            "99.94694948196411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 요약할 문서 FileContent 변수에 할당하기\n",
        "ted_cleaned_FileContent = ted_cleaned_text\n",
        "ted_cleaned_FileContent\n",
        "len(ted_cleaned_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyFW1wNRecHN",
        "outputId": "c944deb2-b73d-4589-a4fa-cb0aa2694a01"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19519"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(ted_cleaned_FileContent)"
      ],
      "metadata": {
        "id": "j6oxtcxRfGsu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8H0g0UdfNPA",
        "outputId": "03d99c2d-9dea-4f35-e281-52de944fe7e8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWmhejiRgRHn",
        "outputId": "bc910c3d-d13d-4b32-cb1d-115f5d9d1b20"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(ted_cleaned_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNF14ASDgSHS",
        "outputId": "36338e25-3b7f-4b73-f4fc-af1b1d444564"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4085"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiiDG2zmgT-j",
        "outputId": "3d5f4735-1aab-489d-977e-30633156e660"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Goldman Sachs CEO said Goldman Sachs workers are the most productive in the world. Peter Bergen: We've stopped asking tough questions about what is the difference between value creation and value extraction, productive and unproductive activities. He argues that we have to have a proper theory of value to answer this question.\n",
            " Adam Smith argued that industrial labor was the source of the value that was getting siphoned out of the economy. Adam Smith showed that 10 specialized workers who had been invested in, in their human capital, could produce 4,800 pins a day, as opposed to one by an unspecialized worker. The big revolution that happened with the current system of economic thinking that we have, which is called \"neoclassical Economics,\" was that the logic completely changed.\n",
            " Up until 1970, most of the financial sector was not even included in GDP. Instead of pausing and asking, \"What is it actually doing?\" -- that was a missed opportunity. This real focus on prices and also share prices has created a huge problem of reinvestment. In the UK, something like between 10 and 20 percent of finance finds its way into industry.\n",
            " We need to rethink how we're measuring output, and how we steer the economy, to which sectors we end up helping, supporting, and also making people feel proud to be part of. We could have had the financial transaction tax, which would have rewarded long-termism, but we didn't decide to do that globally. We should also as a society pause and ask: What Value are we even creating?\n",
            " We should also ask ourselves, had there been a net present value calculation or a cost-benefit analysis done about whether or not to even try to go to the Moon and back again in a generation, we probably wouldn't have started. So Thank God, because I'm an economist, and I can tell you, value is not price. Thank You.\n",
            "84.87279200553894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Goleden Review"
      ],
      "metadata": {
        "id": "ll2ZfzJUjCVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 없이 요약 걸리는 시간\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "summarize(gr_subtitle_merge)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "-l6uZUnPgd3p",
        "outputId": "03c0d9ea-1f7e-4522-ee46-42773957e213"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1370 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1102/3406696842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr_subtitle_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_1102/1975278645.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(subtitle)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msummarized_subtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msummarized_subtitle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         if (\n\u001b[1;32m    140\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_shape, past_key_values_length)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return F.embedding(\n\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약 길이\n",
        "print(len(summarize(gr_subtitle_merge)))"
      ],
      "metadata": {
        "id": "ePUp4lfRjJEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약\n",
        "print(summarize(gr_subtitle_merge))"
      ],
      "metadata": {
        "id": "mVIm8CPkjLbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 후 요약 걸리는 시간\n",
        "start = time.time()\n",
        "\n",
        "summarize(gr_cleaned_text)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "id": "pcHeJzUGjMY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약 길이\n",
        "print(len(summarize(gr_cleaned_text)))"
      ],
      "metadata": {
        "id": "HHbqjY-XjN8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약\n",
        "print(summarize(gr_cleaned_text))"
      ],
      "metadata": {
        "id": "lRxnY6aCjOsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"sshleifer/distilbart-cnn-12-6\" 모델"
      ],
      "metadata": {
        "id": "xgOtCy6-jQXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "gr_FileContent = gr_subtitle_merge\n",
        "gr_FileContent\n",
        "len(gr_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45q5v_PdjRpn",
        "outputId": "87006afb-adc2-4c2e-c991-56cd89fc5c1d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6735"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(gr_FileContent)"
      ],
      "metadata": {
        "id": "EDGcmWa_jUG3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c7CDAzjVcw",
        "outputId": "c94bbc5d-e5a8-48e7-cf7e-7c2d498b8bf7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD5MHXFCjYqe",
        "outputId": "872dbbd8-5744-4dfe-d124-f7b1c84e696a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(gr_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_y0aHzojZE-",
        "outputId": "f2980309-2d82-436a-cce4-2a1bb6191243"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1368"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he5wCU_vjaHZ",
        "outputId": "d563b306-db82-42f3-b031-a5b64aad11f5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots for next week's gallery of snapshots of places you want to visit. Visit CNN iReport.com/Travel next Wednesday for a new gallery of shots of travel next week.\n",
            "9.549052715301514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "gr_cleaned_FileContent = gr_cleaned_text\n",
        "gr_cleaned_FileContent\n",
        "len(gr_cleaned_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6832j7aIjw2S",
        "outputId": "8515974c-0a7d-4cb0-9810-1a9c1c99fda9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6756"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(gr_cleaned_FileContent)"
      ],
      "metadata": {
        "id": "iaBGN1NLj78t"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCo0sBc5j8Vv",
        "outputId": "eba16ffd-6f56-4c41-fa6d-15bdc9d9f0a6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ANVrGhj_ZY",
        "outputId": "65c32f8e-a096-4ef5-f988-2bb39167c113"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1470"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(gr_cleaned_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKnSBLiKj_tg",
        "outputId": "8c8b1498-3d07-4bc0-aa17-3a75a262f5d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1466"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOxA6q7LkArj",
        "outputId": "6e3ce126-96e8-4e2b-90bd-562b840e3216"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The game Optimization service actually download a full list of app signatures from Samsung server when it's running in the background. So we block them from accessing the internet so they cannot download the app name list and then we clear the app data to remove any already downloaded app names. Finally turn off your Wi-fi or data and ending and restart your phone.\n",
            " If we have Gos on, the power is restricted to a very low level, even from the beginning, so we don't see any change in power consumption. With Gos Off, we get more than 10 Fps more and the game indeed run much smoother. We see an overall more than 50 percent power consumption increase, especially for the first two minutes when the device has not started to throttle.\n",
            "33.93785858154297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) WION"
      ],
      "metadata": {
        "id": "y5XVuQeFlpYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 없이 요약 걸리는 시간\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "summarize(wion_subtitle_merge)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci8OBUR_kPwD",
        "outputId": "856e18a1-28b7-4c29-e1a5-9a736e4eecd5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27.419185161590576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약 길이\n",
        "print(len(summarize(wion_subtitle_merge)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5J8BDoNlubY",
        "outputId": "90d72042-db87-4ebc-a5c0-c48a636489a1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 문장 요약\n",
        "print(summarize(wion_subtitle_merge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRmEnuYuluW1",
        "outputId": "f0bc1d7e-45cd-4879-f8d7-7fc6463eebb0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The russia-ukraine war could be headed for a frozen conflict. A frozen conflict is a situation in which active armed conflict has ended but without a formal peace agreement or other political framework in place. frozen conflict occurs in regions of a country no longer controlled by its central authorities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장구별 및 온점처리 후 요약 걸리는 시간\n",
        "start = time.time()\n",
        "\n",
        "summarize(wion_cleaned_text)\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr0173OpluSh",
        "outputId": "50e99e28-bf79-414a-8829-ff29b8e9e442"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27.26302719116211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약 길이\n",
        "print(len(summarize(wion_cleaned_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERiUVDg-luPO",
        "outputId": "d7654197-204c-42f5-de74-f5291316d9e5"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문장 요약\n",
        "print(summarize(wion_cleaned_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJpk_0cXluKn",
        "outputId": "1dc721e0-5aac-4d7e-954e-3847b825eb04"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A frozen conflict is a situation in which active armed conflict has ended, but without a formal peace agreement or other political framework in place. Frozen conflict occurs in regions of a country no longer controlled by its central authorities. This form of conflict was unique to a few former Soviet republics, especially during the collapse of the Soviet Union.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"sshleifer/distilbart-cnn-12-6\" 모델"
      ],
      "metadata": {
        "id": "aRQbndyrl033"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "wion_FileContent = wion_subtitle_merge\n",
        "wion_FileContent\n",
        "len(wion_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cdmbDjjluCi",
        "outputId": "f8b5721f-6dd0-4941-9a7d-9172b3f78524"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2178"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(wion_FileContent)"
      ],
      "metadata": {
        "id": "giyBaU4Rl2aD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_2MpJ18l3Yg",
        "outputId": "2770005c-e8f4-47cd-e5fb-8ecb93e5dd31"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHtnkr7nl6QF",
        "outputId": "fb03f105-0c1b-4394-f61a-6e9dff937248"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "452"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(wion_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTw4GwH9l7OK",
        "outputId": "c4fd66ef-b912-4d19-fc3a-578942490470"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "451"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi5_q1bsl8Np",
        "outputId": "a4c8cdd1-c6ee-4677-c6b9-784e8eb2fdcf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The russia-ukraine war could be headed for a frozen conflict. A frozen conflict is a situation in which active armed conflict has ended but without a formal peace agreement or other political framework in place frozen conflict occurs in regions of a country no longer controlled by its central authorities. These zones then remain under jurisdiction of the separators.\n",
            "14.656025409698486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약할 문서 FileContent 변수에 할당하기\n",
        "wion_cleaned_FileContent = wion_cleaned_text\n",
        "wion_cleaned_FileContent\n",
        "len(wion_cleaned_FileContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skT5E78SnpEe",
        "outputId": "533b6d3e-0c61-475b-b348-0a1dc81d8773"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2207"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(wion_cleaned_FileContent)"
      ],
      "metadata": {
        "id": "F9TFc00ZnzvS"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chunks\n",
        "\n",
        "\n",
        "# initialize\n",
        "\n",
        "length = 0\n",
        "chunk = \"\"\n",
        "chunks = []\n",
        "count = -1\n",
        "for sentence in sentences:\n",
        "  count += 1\n",
        "  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed\n",
        "    chunk += sentence + \" \" # add the sentence to the chunk\n",
        "    length = combined_length # update the length counter\n",
        "\n",
        "    # if it is the last sentence\n",
        "    if count == len(sentences) - 1:\n",
        "      chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "  else: \n",
        "    chunks.append(chunk.strip()) # save the chunk\n",
        "    \n",
        "    # reset \n",
        "    length = 0 \n",
        "    chunk = \"\"\n",
        "\n",
        "    # take care of the overflow sentence\n",
        "    chunk += sentence + \" \"\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snNqqPj9nzsr",
        "outputId": "f75f23ff-bdc1-4a4a-b257-56fa06696044"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With special tokens added\n",
        "sum([len(tokenizer(c).input_ids) for c in chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP3HG69Inzph",
        "outputId": "da1c4805-8d45-4c9f-e368-015a747530ac"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(wion_cleaned_FileContent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-shNncpnzmc",
        "outputId": "843c1dc5-d83a-4fad-e497-1cad7aaa3729"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "440"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inputs\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
        "\n",
        "# Output\n",
        "for input in inputs:\n",
        "  output = model.generate(**input)\n",
        "  print(tokenizer.decode(*output, skip_special_tokens=True))\n",
        "\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tX5zlYHnzj4",
        "outputId": "73894a0e-4a46-4f41-a180-c3bdebfb9909"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A frozen conflict is a situation in which active armed conflict has ended, but without a formal peace agreement or other political framework in place. This form of conflict was unique to a few former Soviet republics, especially during the collapse of the Soviet Union in 1991.. now, Russia is often accused of destabilizing its former Soviet neighbors to keep them in its sphere of influence.\n",
            "12.231482744216919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "386iVE0QnzaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}